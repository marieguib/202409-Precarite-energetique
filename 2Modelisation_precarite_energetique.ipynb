{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Modélisation statistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs\n",
    "!pip install pandas\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install imbalanced-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importation des données avec code_insee en index\n",
    "donnees = pd.read_csv(\"bdd_finale.csv\", sep=',', encoding='utf-8', index_col=0)\n",
    "donnees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Formatage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons pouvoir observer le type des données que nous avons. Cela va nous permettre de déterminer si on a besoin de modifier certaines variables pour qu'elles soient bien en variables catégorielles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des variables en variables catégorielles\n",
    "donnees[\"beneficiaire_trans_eco\"] = donnees[\"beneficiaire_trans_eco\"].astype('category')\n",
    "donnees[\"ecoquartiers\"] = donnees[\"ecoquartiers\"].astype('category')\n",
    "donnees[\"beneficiaire_prog\"] = donnees[\"beneficiaire_prog\"].astype('category')\n",
    "donnees[\"gridens7\"] = donnees[\"gridens7\"].astype('category')\n",
    "donnees[\"departement\"] = donnees[\"departement\"].astype('category')\n",
    "donnees[\"gare_tgv\"] = donnees[\"gare_tgv\"].astype('category')\n",
    "donnees[\"CSP_maire\"] = donnees[\"CSP_maire\"].astype('category')\n",
    "\n",
    "\n",
    "# Cas de la variable climat \n",
    "donnees = pd.get_dummies(donnees, columns = [\"climat\"]) # On crée des variables binaires pour la variable climat car on a des chaînes de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons observer si on a des données manquantes dans notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Séparation de la variable à expliquer et des variables explicatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir X et y pour avoir les variables explicatives et la variable à expliquer beneficiaire_trans_eco\n",
    "X = donnees.drop(columns=['beneficiaire_trans_eco'])\n",
    "\n",
    "# Variable à expliquer : beneficiaire_trans_eco\n",
    "y = donnees[\"beneficiaire_trans_eco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Convertir les listes en tableaux numpy\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions de X:\", X.shape)\n",
    "print(\"Dimensions de y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord diviser notre jeu de données en jeux d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2.1. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Construction des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir notre modèle de régression logistique, sans pénalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "etapes_reg_log_oversampler = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('oversampler', RandomOverSampler()), # Sur-échantillonnage\n",
    "        ('reg_log', LogisticRegression()), # Régression logistique\n",
    "        ]\n",
    "\n",
    "etapes_reg_log_smote = [('std_scaler', StandardScaler()), # Standardisation des variables pour ne pas donner plus d'importance à une variable qu'à une autre\n",
    "        ('smote', SMOTE()), # Sur-échantillonnage\n",
    "        ('reg_log', LogisticRegression()), # Régression logistique\n",
    "        ]\n",
    "\n",
    "etapes_reg_log_adasyn = [('std_scaler', StandardScaler()), # Standardisation des variables pour ne pas donner plus d'importance à une variable qu'à une autre\n",
    "        ('adasyn', ADASYN()), # Sur-échantillonnage\n",
    "        ('reg_log', LogisticRegression()), # Régression logistique\n",
    "        ]\n",
    "\n",
    "etapes_reg_log_bordeline_smote = [('std_scaler', StandardScaler()), # Standardisation des variables pour ne pas donner plus d'importance à une variable qu'à une autre\n",
    "        ('bordeline_smote', BorderlineSMOTE()), # Sur-échantillonnage\n",
    "        ('reg_log', LogisticRegression()), # Régression logistique\n",
    "        ]\n",
    "\n",
    "\n",
    "etapes_reg_log = [('std_scaler', StandardScaler(), \n",
    "                   ('reg_log', LogisticRegression())  \n",
    "    \n",
    ")]\n",
    "# Création des pipelines\n",
    "modele_reg_log_smote  = Pipeline(steps=etapes_reg_log_smote) \n",
    "modele_reg_log_oversampler  = Pipeline(steps=etapes_reg_log_oversampler) \n",
    "modele_reg_log_adasyn  = Pipeline(steps=etapes_reg_log_adasyn)\n",
    "modele_reg_log_bordeline_smote  = Pipeline(steps=etapes_reg_log_bordeline_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne le modèle sur les données pour estimer les coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_reg_log_smote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_reg_log_oversampler.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_reg_log_adasyn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_reg_log_bordeline_smote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à prédire de nouvelles données grâce aux données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_reg_log_smote = modele_reg_log_smote.predict_proba(X_test)[:,1]\n",
    "y_pred_reg_log_smote = modele_reg_log_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_reg_log_oversampler = modele_reg_log_oversampler.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_oversampler = modele_reg_log_oversampler.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_reg_log_adasyn = modele_reg_log_adasyn.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_adasyn = modele_reg_log_adasyn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_reg_log_bordeline_smote = modele_reg_log_bordeline_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_bordeline_smote = modele_reg_log_bordeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Mesure de la performance des modèles initiaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant évaluer la performance de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "\n",
    "# Calcul des métriques\n",
    "auc_roc_reg_log_smote = roc_auc_score(y_test, y_proba_reg_log_smote)\n",
    "auc_pr_reg_log_smote  = average_precision_score(y_test, y_proba_reg_log_smote)\n",
    "log_loss_value_reg_log_smote  = log_loss(y_test, y_proba_reg_log_smote)\n",
    "precision_reg_log_smote  = precision_score(y_test, y_pred_reg_log_smote)\n",
    "recall_reg_log_smote  = recall_score(y_test, y_pred_reg_log_smote)\n",
    "f1_reg_log_smote = f1_score(y_test, y_pred_reg_log_smote)\n",
    "mcc_reg_log_smote  = matthews_corrcoef(y_test, y_pred_reg_log_smote)\n",
    "balanced_acc_reg_log_smote  = balanced_accuracy_score(y_test, y_pred_reg_log_smote)\n",
    "specificity_reg_log_smote  = confusion_matrix(y_test, y_pred_reg_log_smote)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_smote)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_smote)[0, 1])\n",
    "cohen_kappa_reg_log_smote  = cohen_kappa_score(y_test, y_pred_reg_log_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "# Calcul des métriques\n",
    "auc_roc_reg_log_oversampler = roc_auc_score(y_test, y_proba_reg_log_oversampler)\n",
    "auc_pr_reg_log_oversampler = average_precision_score(y_test, y_proba_reg_log_oversampler)\n",
    "log_loss_value_reg_log_oversampler = log_loss(y_test, y_proba_reg_log_oversampler)\n",
    "precision_reg_log_oversampler = precision_score(y_test, y_pred_reg_log_oversampler)\n",
    "recall_reg_log_oversampler = recall_score(y_test, y_pred_reg_log_oversampler)\n",
    "f1_reg_log_oversampler = f1_score(y_test, y_pred_reg_log_oversampler)\n",
    "mcc_reg_log_oversampler = matthews_corrcoef(y_test, y_pred_reg_log_oversampler)\n",
    "balanced_acc_reg_log_oversampler = balanced_accuracy_score(y_test, y_pred_reg_log_oversampler)\n",
    "specificity_reg_log_oversampler = confusion_matrix(y_test, y_pred_reg_log_oversampler)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_oversampler)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_oversampler)[0, 1])\n",
    "cohen_kappa_reg_log_oversampler = cohen_kappa_score(y_test, y_pred_reg_log_oversampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "# Calcul des métriques\n",
    "auc_roc_reg_log_adasyn = roc_auc_score(y_test, y_proba_reg_log_adasyn)\n",
    "auc_pr_reg_log_adasyn = average_precision_score(y_test, y_proba_reg_log_adasyn)\n",
    "log_loss_value_reg_log_adasyn = log_loss(y_test, y_proba_reg_log_adasyn)\n",
    "precision_reg_log_adasyn = precision_score(y_test, y_pred_reg_log_adasyn)\n",
    "recall_reg_log_adasyn = recall_score(y_test, y_pred_reg_log_adasyn)\n",
    "f1_reg_log_adasyn = f1_score(y_test, y_pred_reg_log_adasyn)\n",
    "mcc_reg_log_adasyn = matthews_corrcoef(y_test, y_pred_reg_log_adasyn)\n",
    "balanced_acc_reg_log_adasyn = balanced_accuracy_score(y_test, y_pred_reg_log_adasyn)\n",
    "specificity_reg_log_adasyn = confusion_matrix(y_test, y_pred_reg_log_adasyn)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_adasyn)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_adasyn)[0, 1])\n",
    "cohen_kappa_reg_log_adasyn = cohen_kappa_score(y_test, y_pred_reg_log_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "auc_roc_reg_log_borderline_smote = roc_auc_score(y_test, y_proba_reg_log_bordeline_smote)\n",
    "auc_pr_reg_log_borderline_smote = average_precision_score(y_test, y_proba_reg_log_bordeline_smote)\n",
    "log_loss_value_reg_log_borderline_smote = log_loss(y_test, y_proba_reg_log_bordeline_smote)\n",
    "precision_reg_log_borderline_smote = precision_score(y_test, y_pred_reg_log_bordeline_smote)\n",
    "recall_reg_log_borderline_smote = recall_score(y_test, y_pred_reg_log_bordeline_smote)\n",
    "f1_reg_log_borderline_smote = f1_score(y_test, y_pred_reg_log_bordeline_smote)\n",
    "mcc_reg_log_borderline_smote = matthews_corrcoef(y_test, y_pred_reg_log_bordeline_smote)\n",
    "balanced_acc_reg_log_borderline_smote = balanced_accuracy_score(y_test, y_pred_reg_log_bordeline_smote)\n",
    "specificity_reg_log_borderline_smote = confusion_matrix(y_test, y_pred_reg_log_bordeline_smote)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_bordeline_smote)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_bordeline_smote)[0, 1])\n",
    "cohen_kappa_reg_log_borderline_smote = cohen_kappa_score(y_test, y_pred_reg_log_bordeline_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_reg_log_smote, tpr_reg_log_smote, _ = roc_curve(y_test, y_proba_reg_log_smote)\n",
    "fpr_reg_log_oversampler, tpr_reg_log_oversampler, _ = roc_curve(y_test, y_proba_reg_log_oversampler)\n",
    "fpr_reg_log_adasyn, tpr_reg_log_adasyn, _ = roc_curve(y_test, y_proba_reg_log_adasyn)\n",
    "fpr_reg_log_bordeline_smote, tpr_reg_log_bordeline_smote, _ = roc_curve(y_test, y_proba_reg_log_bordeline_smote)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_reg_log_smote, tpr_reg_log_smote, label='Régression logistique avec SMOTE')\n",
    "plt.plot(fpr_reg_log_oversampler, tpr_reg_log_oversampler, label='Régression logistique avec RandomOverSampler')\n",
    "plt.plot(fpr_reg_log_adasyn, tpr_reg_log_adasyn, label='Régression logistique avec ADASYN')\n",
    "plt.plot(fpr_reg_log_bordeline_smote, tpr_reg_log_bordeline_smote, label='Régression logistique avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes PR\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_reg_log_smote, recall_reg_log_smote, _ = precision_recall_curve(y_test, y_proba_reg_log_smote)\n",
    "precision_reg_log_oversampler, recall_reg_log_oversampler, _ = precision_recall_curve(y_test, y_proba_reg_log_oversampler)\n",
    "precision_reg_log_adasyn, recall_reg_log_adasyn, _ = precision_recall_curve(y_test, y_proba_reg_log_adasyn)\n",
    "precision_reg_log_bordeline_smote, recall_reg_log_bordeline_smote, _ = precision_recall_curve(y_test, y_proba_reg_log_bordeline_smote)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall_reg_log_smote, precision_reg_log_smote, label='Régression logistique avec SMOTE')\n",
    "plt.plot(recall_reg_log_oversampler, precision_reg_log_oversampler, label='Régression logistique avec RandomOverSampler')\n",
    "plt.plot(recall_reg_log_adasyn, precision_reg_log_adasyn, label='Régression logistique avec ADASYN')\n",
    "plt.plot(recall_reg_log_bordeline_smote, precision_reg_log_bordeline_smote, label='Régression logistique avec BorderlineSMOTE')\n",
    "plt.xlabel('Rappel')\n",
    "plt.ylabel('Précision')\n",
    "plt.title('Courbes PR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultats \n",
    "\n",
    "resultats_reg_log = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_reg_log_smote, auc_roc_reg_log_oversampler, auc_roc_reg_log_adasyn, auc_roc_reg_log_borderline_smote],\n",
    "    \"AUC-PR\": [auc_pr_reg_log_smote, auc_pr_reg_log_oversampler, auc_pr_reg_log_adasyn, auc_pr_reg_log_borderline_smote],\n",
    "    \"Log Loss\": [log_loss_value_reg_log_smote, log_loss_value_reg_log_oversampler, log_loss_value_reg_log_adasyn, log_loss_value_reg_log_borderline_smote],\n",
    "   # \"Précision\": [precision_reg_log_smote, precision_reg_log_oversampler, precision_reg_log_adasyn, precision_reg_log_borderline_smote],\n",
    "    #\"Rappel\": [recall_reg_log_smote, recall_reg_log_oversampler, recall_reg_log_adasyn, recall_reg_log_borderline_smote],\n",
    "    \"F1 Score\": [f1_reg_log_smote, f1_reg_log_oversampler, f1_reg_log_adasyn, f1_reg_log_borderline_smote],\n",
    "    \"MCC\": [mcc_reg_log_smote, mcc_reg_log_oversampler, mcc_reg_log_adasyn, mcc_reg_log_borderline_smote],\n",
    "    \"Accuracy\": [balanced_acc_reg_log_smote, balanced_acc_reg_log_oversampler, balanced_acc_reg_log_adasyn, balanced_acc_reg_log_borderline_smote],\n",
    "    \"Spécificité\": [specificity_reg_log_smote, specificity_reg_log_oversampler, specificity_reg_log_adasyn, specificity_reg_log_borderline_smote],\n",
    "    \"Kappa de Cohen\": [cohen_kappa_reg_log_smote, cohen_kappa_reg_log_oversampler, cohen_kappa_reg_log_adasyn, cohen_kappa_reg_log_borderline_smote]\n",
    "}, index=[\"Reg log - SMOTE\", \"Reg log - RandomOverSampler\", \"Reg log - ADASYN\", \"Reg log - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_reg_log = resultats_reg_log.round(3)\n",
    "resultats_reg_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ajustement des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la régression logistique, les hyperparamètres les plus courants à optimiser sont :\n",
    "- C : Ce paramètre de régularisation inverse. Une valeur plus petite indique une régularisation plus forte.\n",
    "- solver : L'algorithme utilisé pour l'optimisation. Les choix courants sont 'liblinear', 'lbfgs', 'saga', etc.\n",
    "- penalty : Le type de régularisation à utiliser ('l1', 'l2', 'elasticnet', 'none')\n",
    "- fonction de lien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement de chaque modèle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Définir les paramètres de la grille\n",
    "grille_param = {\n",
    "    'reg_log__C': [0.01, 0.1, 1, 10, 100], # Inverse de la force de régularisation : pas d'utilité pour la régression logistique sans régularisation\n",
    "    'reg_log__penalty': ['l1', 'l2', 'elasticnet', 'None'], # Type de régularisation\n",
    "}\n",
    "\n",
    "# Choix des métriques pour l'optimisation\n",
    "scoring = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Recall': make_scorer(recall_score),\n",
    "    'AUC': 'roc_auc'\n",
    "}\n",
    "\n",
    "\n",
    "# Création de la grille de recherche\n",
    "grille_recherche_reg_log_smote = GridSearchCV(modele_reg_log_smote, grille_param, cv=5, scoring=scoring, refit='AUC')\n",
    "grille_recherche_reg_log_oversampler = GridSearchCV(modele_reg_log_oversampler, grille_param, cv=5, scoring=scoring, refit='AUC')\n",
    "grille_recherche_reg_log_adasyn = GridSearchCV(modele_reg_log_adasyn, grille_param, cv=5, scoring=scoring, refit='AUC')\n",
    "grille_recherche_reg_log_bordeline_smote = GridSearchCV(modele_reg_log_bordeline_smote, grille_param, cv=5, scoring=scoring, refit='AUC')\n",
    "\n",
    "# Ajustement de la grille de recherche\n",
    "grille_recherche_reg_log_smote.fit(X_train, y_train)\n",
    "grille_recherche_reg_log_oversampler.fit(X_train, y_train)\n",
    "grille_recherche_reg_log_adasyn.fit(X_train, y_train)\n",
    "grille_recherche_reg_log_bordeline_smote.fit(X_train, y_train)\n",
    "\n",
    "# Récupération des résultats de la grille de recherche\n",
    "resultats_grille_reg_log_smote = grille_recherche_reg_log_smote.cv_results_\n",
    "resultats_grille_reg_log_oversampler = grille_recherche_reg_log_oversampler.cv_results_\n",
    "resultats_grille_reg_log_adasyn = grille_recherche_reg_log_adasyn.cv_results_\n",
    "resultats_grille_reg_log_bordeline_smote = grille_recherche_reg_log_bordeline_smote.cv_results_\n",
    "\n",
    "# Récupération des meilleurs paramètres\n",
    "meilleurs_parametres_reg_log_smote = grille_recherche_reg_log_smote.best_params_\n",
    "meilleurs_parametres_reg_log_oversampler = grille_recherche_reg_log_oversampler.best_params_\n",
    "meilleurs_parametres_reg_log_adasyn = grille_recherche_reg_log_adasyn.best_params_\n",
    "meilleurs_parametres_reg_log_bordeline_smote = grille_recherche_reg_log_bordeline_smote.best_params_\n",
    "\n",
    "# Récupération des meilleurs scores\n",
    "meilleur_score_reg_log_smote = grille_recherche_reg_log_smote.best_score_\n",
    "meilleur_score_reg_log_oversampler = grille_recherche_reg_log_oversampler.best_score_\n",
    "meilleur_score_reg_log_adasyn = grille_recherche_reg_log_adasyn.best_score_\n",
    "meilleur_score_reg_log_bordeline_smote = grille_recherche_reg_log_bordeline_smote.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement des modèles\n",
    "modele_reg_log_smote_cv = grille_recherche_reg_log_smote.best_estimator_\n",
    "modele_reg_log_oversampler_cv = grille_recherche_reg_log_oversampler.best_estimator_\n",
    "modele_reg_log_adasyn_cv = grille_recherche_reg_log_adasyn.best_estimator_\n",
    "modele_reg_log_bordeline_smote_cv = grille_recherche_reg_log_bordeline_smote.best_estimator_\n",
    "\n",
    "# Prédiction sur l'échantillon test\n",
    "y_proba_reg_log_smote_cv = modele_reg_log_smote_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_smote_cv = modele_reg_log_smote_cv.predict(X_test)\n",
    "\n",
    "y_proba_reg_log_oversampler_cv = modele_reg_log_oversampler_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_oversampler_cv = modele_reg_log_oversampler_cv.predict(X_test)\n",
    "\n",
    "y_proba_reg_log_adasyn_cv = modele_reg_log_adasyn_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_adasyn_cv = modele_reg_log_adasyn_cv.predict(X_test)\n",
    "\n",
    "y_proba_reg_log_bordeline_smote_cv = modele_reg_log_bordeline_smote_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg_log_bordeline_smote_cv = modele_reg_log_bordeline_smote_cv.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Mesure de la performance des modèles optimisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques pour chaque modèle\n",
    "\n",
    "# Calcul des métriques\n",
    "auc_roc_reg_log_smote_cv = roc_auc_score(y_test, y_proba_reg_log_smote_cv)\n",
    "auc_pr_reg_log_smote_cv = average_precision_score(y_test, y_proba_reg_log_smote_cv)\n",
    "log_loss_value_reg_log_smote_cv = log_loss(y_test, y_proba_reg_log_smote_cv)\n",
    "precision_reg_log_smote_cv = precision_score(y_test, y_pred_reg_log_smote_cv)\n",
    "recall_reg_log_smote_cv = recall_score(y_test, y_pred_reg_log_smote_cv)\n",
    "f1_reg_log_smote_cv = f1_score(y_test, y_pred_reg_log_smote_cv)\n",
    "mcc_reg_log_smote_cv = matthews_corrcoef(y_test, y_pred_reg_log_smote_cv)\n",
    "balanced_acc_reg_log_smote_cv = balanced_accuracy_score(y_test, y_pred_reg_log_smote_cv)\n",
    "specificity_reg_log_smote_cv = confusion_matrix(y_test, y_pred_reg_log_smote_cv)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_smote_cv)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_smote_cv)[0, 1])\n",
    "cohen_kappa_reg_log_smote_cv = cohen_kappa_score(y_test, y_pred_reg_log_smote_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_reg_log_oversampler_cv = roc_auc_score(y_test, y_proba_reg_log_oversampler_cv)\n",
    "auc_pr_reg_log_oversampler_cv = average_precision_score(y_test, y_proba_reg_log_oversampler_cv)\n",
    "log_loss_value_reg_log_oversampler_cv = log_loss(y_test, y_proba_reg_log_oversampler_cv)\n",
    "precision_reg_log_oversampler_cv = precision_score(y_test, y_pred_reg_log_oversampler_cv)\n",
    "recall_reg_log_oversampler_cv = recall_score(y_test, y_pred_reg_log_oversampler_cv)\n",
    "f1_reg_log_oversampler_cv = f1_score(y_test, y_pred_reg_log_oversampler_cv)\n",
    "mcc_reg_log_oversampler_cv = matthews_corrcoef(y_test, y_pred_reg_log_oversampler_cv)\n",
    "balanced_acc_reg_log_oversampler_cv = balanced_accuracy_score(y_test, y_pred_reg_log_oversampler_cv)\n",
    "specificity_reg_log_oversampler_cv = confusion_matrix(y_test, y_pred_reg_log_oversampler_cv)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_oversampler_cv)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_oversampler_cv)[0, 1])\n",
    "cohen_kappa_reg_log_oversampler_cv = cohen_kappa_score(y_test, y_pred_reg_log_oversampler_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_reg_log_adasyn_cv = roc_auc_score(y_test, y_proba_reg_log_adasyn_cv)\n",
    "auc_pr_reg_log_adasyn_cv = average_precision_score(y_test, y_proba_reg_log_adasyn_cv)   \n",
    "log_loss_value_reg_log_adasyn_cv = log_loss(y_test, y_proba_reg_log_adasyn_cv)\n",
    "precision_reg_log_adasyn_cv = precision_score(y_test, y_pred_reg_log_adasyn_cv)\n",
    "recall_reg_log_adasyn_cv = recall_score(y_test, y_pred_reg_log_adasyn_cv)\n",
    "f1_reg_log_adasyn_cv = f1_score(y_test, y_pred_reg_log_adasyn_cv)\n",
    "mcc_reg_log_adasyn_cv = matthews_corrcoef(y_test, y_pred_reg_log_adasyn_cv)\n",
    "balanced_acc_reg_log_adasyn_cv = balanced_accuracy_score(y_test, y_pred_reg_log_adasyn_cv)\n",
    "specificity_reg_log_adasyn_cv = confusion_matrix(y_test, y_pred_reg_log_adasyn_cv)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_adasyn_cv)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_adasyn_cv)[0, 1])\n",
    "cohen_kappa_reg_log_adasyn_cv = cohen_kappa_score(y_test, y_pred_reg_log_adasyn_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_reg_log_borderline_smote = roc_auc_score(y_test, y_proba_reg_log_bordeline_smote_cv)\n",
    "auc_pr_reg_log_borderline_smote = average_precision_score(y_test, y_proba_reg_log_bordeline_smote_cv)\n",
    "log_loss_value_reg_log_borderline_smote = log_loss(y_test, y_proba_reg_log_bordeline_smote_cv)\n",
    "precision_reg_log_borderline_smote = precision_score(y_test, y_pred_reg_log_bordeline_smote_cv)\n",
    "recall_reg_log_borderline_smote = recall_score(y_test, y_pred_reg_log_bordeline_smote_cv)\n",
    "f1_reg_log_borderline_smote = f1_score(y_test, y_pred_reg_log_bordeline_smote_cv)\n",
    "mcc_reg_log_borderline_smote = matthews_corrcoef(y_test, y_pred_reg_log_bordeline_smote_cv)\n",
    "balanced_acc_reg_log_borderline_smote = balanced_accuracy_score(y_test, y_pred_reg_log_bordeline_smote_cv)\n",
    "specificity_reg_log_borderline_smote = confusion_matrix(y_test, y_pred_reg_log_bordeline_smote_cv)[0, 0] / (confusion_matrix(y_test, y_pred_reg_log_bordeline_smote_cv)[0, 0] + confusion_matrix(y_test, y_pred_reg_log_bordeline_smote_cv)[0, 1])\n",
    "cohen_kappa_reg_log_borderline_smote = cohen_kappa_score(y_test, y_pred_reg_log_bordeline_smote_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_reg_log_cv = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_reg_log_smote_cv, auc_roc_reg_log_oversampler_cv, auc_roc_reg_log_adasyn_cv, auc_roc_reg_log_borderline_smote],\n",
    "    \"AUC-PR\": [auc_pr_reg_log_smote_cv, auc_pr_reg_log_oversampler_cv, auc_pr_reg_log_adasyn_cv, auc_pr_reg_log_borderline_smote],\n",
    "    \"Log Loss\": [log_loss_value_reg_log_smote_cv, log_loss_value_reg_log_oversampler_cv, log_loss_value_reg_log_adasyn_cv, log_loss_value_reg_log_borderline_smote],\n",
    "    \"Précision\": [precision_reg_log_smote_cv, precision_reg_log_oversampler_cv, precision_reg_log_adasyn_cv, precision_reg_log_borderline_smote],\n",
    "    \"Rappel\": [recall_reg_log_smote_cv, recall_reg_log_oversampler_cv, recall_reg_log_adasyn_cv, recall_reg_log_borderline_smote],\n",
    "    \"F1 Score\": [f1_reg_log_smote_cv, f1_reg_log_oversampler_cv, f1_reg_log_adasyn_cv, f1_reg_log_borderline_smote],\n",
    "    \"MCC\": [mcc_reg_log_smote_cv, mcc_reg_log_oversampler_cv, mcc_reg_log_adasyn_cv, mcc_reg_log_borderline_smote],\n",
    "    \"Accuracy\": [balanced_acc_reg_log_smote_cv, balanced_acc_reg_log_oversampler_cv, balanced_acc_reg_log_adasyn_cv, balanced_acc_reg_log_borderline_smote],\n",
    "    \"Spécificité\": [specificity_reg_log_smote_cv, specificity_reg_log_oversampler_cv, specificity_reg_log_adasyn_cv, specificity_reg_log_borderline_smote],\n",
    "    \"Kappa de Cohen\": [cohen_kappa_reg_log_smote_cv, cohen_kappa_reg_log_oversampler_cv, cohen_kappa_reg_log_adasyn_cv, cohen_kappa_reg_log_borderline_smote]\n",
    "}, index=[\"Reg log - SMOTE\", \"Reg log - RandomOverSampler\", \"Reg log - ADASYN\", \"Reg log - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_reg_log_cv = resultats_reg_log_cv.round(3)\n",
    "resultats_reg_log_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes ROC\n",
    "\n",
    "fpr_reg_log_smote_cv, tpr_reg_log_smote_cv, _ = roc_curve(y_test, y_proba_reg_log_smote_cv)\n",
    "fpr_reg_log_oversampler_cv, tpr_reg_log_oversampler_cv, _ = roc_curve(y_test, y_proba_reg_log_oversampler_cv)\n",
    "fpr_reg_log_adasyn_cv, tpr_reg_log_adasyn_cv, _ = roc_curve(y_test, y_proba_reg_log_adasyn_cv)\n",
    "fpr_reg_log_bordeline_smote_cv, tpr_reg_log_bordeline_smote_cv, _ = roc_curve(y_test, y_proba_reg_log_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_reg_log_smote_cv, tpr_reg_log_smote_cv, label='Régression logistique avec SMOTE')\n",
    "plt.plot(fpr_reg_log_oversampler_cv, tpr_reg_log_oversampler_cv, label='Régression logistique avec RandomOverSampler')\n",
    "plt.plot(fpr_reg_log_adasyn_cv, tpr_reg_log_adasyn_cv, label='Régression logistique avec ADASYN')\n",
    "plt.plot(fpr_reg_log_bordeline_smote_cv, tpr_reg_log_bordeline_smote_cv, label='Régression logistique avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes PR\n",
    "\n",
    "precision_reg_log_smote_cv, recall_reg_log_smote_cv, _ = precision_recall_curve(y_test, y_proba_reg_log_smote_cv)\n",
    "precision_reg_log_oversampler_cv, recall_reg_log_oversampler_cv, _ = precision_recall_curve(y_test, y_proba_reg_log_oversampler_cv)\n",
    "precision_reg_log_adasyn_cv, recall_reg_log_adasyn_cv, _ = precision_recall_curve(y_test, y_proba_reg_log_adasyn_cv)\n",
    "precision_reg_log_bordeline_smote_cv, recall_reg_log_bordeline_smote_cv, _ = precision_recall_curve(y_test, y_proba_reg_log_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall_reg_log_smote_cv, precision_reg_log_smote_cv, label='Régression logistique avec SMOTE')\n",
    "plt.plot(recall_reg_log_oversampler_cv, precision_reg_log_oversampler_cv, label='Régression logistique avec RandomOverSampler')\n",
    "plt.plot(recall_reg_log_adasyn_cv, precision_reg_log_adasyn_cv, label='Régression logistique avec ADASYN')\n",
    "plt.plot(recall_reg_log_bordeline_smote_cv, precision_reg_log_bordeline_smote_cv, label='Régression logistique avec BorderlineSMOTE')\n",
    "\n",
    "plt.xlabel('Rappel')\n",
    "plt.ylabel('Précision')\n",
    "plt.title('Courbes PR')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "\n",
    "# Récupération des coefficients de la régression logistique\n",
    "coefficients_reg_log_smote = modele_reg_log_smote_cv.named_steps['reg_log'].coef_[0]\n",
    "coefficients_reg_log_oversampler = modele_reg_log_oversampler_cv.named_steps['reg_log'].coef_[0]\n",
    "coefficients_reg_log_adasyn = modele_reg_log_adasyn_cv.named_steps['reg_log'].coef_[0]\n",
    "coefficients_reg_log_bordeline_smote = modele_reg_log_bordeline_smote_cv.named_steps['reg_log'].coef_[0]\n",
    "\n",
    "# Récupération des noms des variables\n",
    "noms_variables = X.columns\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "resultats_importance_variables_reg_log_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Coefficient\": coefficients_reg_log_smote\n",
    "})\n",
    "\n",
    "resultats_importance_variables_reg_log_oversampler = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Coefficient\": coefficients_reg_log_oversampler\n",
    "})\n",
    "\n",
    "resultats_importance_variables_reg_log_adasyn = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Coefficient\": coefficients_reg_log_adasyn\n",
    "})\n",
    "\n",
    "resultats_importance_variables_reg_log_bordeline_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Coefficient\": coefficients_reg_log_bordeline_smote\n",
    "})\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "resultats_importance_variables_reg_log_smote = resultats_importance_variables_reg_log_smote.sort_values(by=\"Coefficient\", ascending=False)\n",
    "resultats_importance_variables_reg_log_oversampler = resultats_importance_variables_reg_log_oversampler.sort_values(by=\"Coefficient\", ascending=False)\n",
    "resultats_importance_variables_reg_log_adasyn = resultats_importance_variables_reg_log_adasyn.sort_values(by=\"Coefficient\", ascending=False)\n",
    "resultats_importance_variables_reg_log_bordeline_smote = resultats_importance_variables_reg_log_bordeline_smote.sort_values(by=\"Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_reg_log_oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_reg_log_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_reg_log_bordeline_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique pour l'importance des variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_reg_log_smote[\"Variable\"], resultats_importance_variables_reg_log_smote[\"Coefficient\"])\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"Importance des variables pour la régression logistique avec SMOTE\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_reg_log_oversampler[\"Variable\"], resultats_importance_variables_reg_log_oversampler[\"Coefficient\"])\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"Importance des variables pour la régression logistique avec RandomOverSampler\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_reg_log_adasyn[\"Variable\"], resultats_importance_variables_reg_log_adasyn[\"Coefficient\"])\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"Importance des variables pour la régression logistique avec ADASYN\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_reg_log_bordeline_smote[\"Variable\"], resultats_importance_variables_reg_log_bordeline_smote[\"Coefficient\"])\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"Importance des variables pour la régression logistique avec BorderlineSMOTE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2.3. Abres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Construction des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle d'arbres de décision avec rééquilibrage des classes\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "etapes_arbre_oversampler = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('oversampler', RandomOverSampler()), # Sur-échantillonnage\n",
    "        ('arbre', DecisionTreeClassifier(max_depth=5)), # Arbre de décision\n",
    "        ]\n",
    "\n",
    "etapes_arbre_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('smote', SMOTE()), # Sur-échantillonnage\n",
    "        ('arbre', DecisionTreeClassifier(max_depth=5)), # Arbre de décision\n",
    "        ]\n",
    "\n",
    "etapes_arbre_adasyn = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('adasyn', ADASYN()), # Sur-échantillonnage\n",
    "        ('arbre', DecisionTreeClassifier(max_depth=5)), # Arbre de décision\n",
    "        ]\n",
    "\n",
    "etapes_arbre_bordeline_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('bordeline_smote', BorderlineSMOTE()), # Sur-échantillonnage\n",
    "        ('arbre', DecisionTreeClassifier(max_depth=5)), # Arbre de décision\n",
    "        ]\n",
    "\n",
    "\n",
    "# Justification de max_depth=5 : pour éviter l'overfitting\n",
    "# On a choisi la valeur 5 car c'est une valeur standard qui permet de ne pas trop complexifier l'arbre de décision\n",
    "\n",
    "modele_arbre_oversampler = Pipeline(steps=etapes_arbre_oversampler) # Création du pipeline\n",
    "modele_arbre_smote = Pipeline(steps=etapes_arbre_smote) # Création du pipeline\n",
    "modele_arbre_adasyn = Pipeline(steps=etapes_arbre_adasyn) # Création du pipeline\n",
    "modele_arbre_bordeline_smote = Pipeline(steps=etapes_arbre_bordeline_smote) # Création du pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_arbre_oversampler.fit(X_train, y_train) # Entraînement\n",
    "modele_arbre_smote.fit(X_train, y_train) # Entraînement\n",
    "modele_arbre_adasyn.fit(X_train, y_train) # Entraînement\n",
    "modele_arbre_bordeline_smote.fit(X_train, y_train) # Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_arbre_oversampler = modele_arbre_oversampler.predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_oversampler = modele_arbre_oversampler.predict(X_test)\n",
    "\n",
    "y_proba_arbre_smote = modele_arbre_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_smote = modele_arbre_smote.predict(X_test)\n",
    "\n",
    "y_proba_arbre_adasyn = modele_arbre_adasyn.predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_adasyn = modele_arbre_adasyn.predict(X_test)\n",
    "\n",
    "y_proba_arbre_bordeline_smote = modele_arbre_bordeline_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_bordeline_smote = modele_arbre_bordeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Mesure de la performance des modèles initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "auc_roc_arbre_oversampler = roc_auc_score(y_test, y_proba_arbre_oversampler)\n",
    "auc_pr_arbre_oversampler = average_precision_score(y_test, y_proba_arbre_oversampler)\n",
    "log_loss_value_arbre_oversampler = log_loss(y_test, y_proba_arbre_oversampler)\n",
    "precision_arbre_oversampler = precision_score(y_test, y_pred_arbre_oversampler)\n",
    "recall_arbre_oversampler = recall_score(y_test, y_pred_arbre_oversampler)\n",
    "f1_arbre_oversampler = f1_score(y_test, y_pred_arbre_oversampler)\n",
    "mcc_arbre_oversampler = matthews_corrcoef(y_test, y_pred_arbre_oversampler)\n",
    "balanced_acc_arbre_oversampler = balanced_accuracy_score(y_test, y_pred_arbre_oversampler)\n",
    "specificity_arbre_oversampler = confusion_matrix(y_test, y_pred_arbre_oversampler)[0, 0] / (confusion_matrix(y_test, y_pred_arbre_oversampler)[0, 0] + confusion_matrix(y_test, y_pred_arbre_oversampler)[0, 1])\n",
    "cohen_kappa_arbre_oversampler = cohen_kappa_score(y_test, y_pred_arbre_oversampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_arbre_smote = roc_auc_score(y_test, y_proba_arbre_smote)\n",
    "auc_pr_arbre_smote = average_precision_score(y_test, y_proba_arbre_smote)\n",
    "log_loss_value_arbre_smote = log_loss(y_test, y_proba_arbre_smote)\n",
    "precision_arbre_smote = precision_score(y_test, y_pred_arbre_smote)\n",
    "recall_arbre_smote = recall_score(y_test, y_pred_arbre_smote)\n",
    "f1_arbre_smote = f1_score(y_test, y_pred_arbre_smote)\n",
    "mcc_arbre_smote = matthews_corrcoef(y_test, y_pred_arbre_smote)\n",
    "balanced_acc_arbre_smote = balanced_accuracy_score(y_test, y_pred_arbre_smote)\n",
    "specificity_arbre_smote = confusion_matrix(y_test, y_pred_arbre_smote)[0, 0] / (confusion_matrix(y_test, y_pred_arbre_smote)[0, 0] + confusion_matrix(y_test, y_pred_arbre_smote)[0, 1])\n",
    "cohen_kappa_arbre_smote = cohen_kappa_score(y_test, y_pred_arbre_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_arbre_adasyn = roc_auc_score(y_test, y_proba_arbre_adasyn)\n",
    "auc_pr_arbre_adasyn = average_precision_score(y_test, y_proba_arbre_adasyn)\n",
    "log_loss_value_arbre_adasyn = log_loss(y_test, y_proba_arbre_adasyn)\n",
    "precision_arbre_adasyn = precision_score(y_test, y_pred_arbre_adasyn)\n",
    "recall_arbre_adasyn = recall_score(y_test, y_pred_arbre_adasyn)\n",
    "f1_arbre_adasyn = f1_score(y_test, y_pred_arbre_adasyn)\n",
    "mcc_arbre_adasyn = matthews_corrcoef(y_test, y_pred_arbre_adasyn)\n",
    "balanced_acc_arbre_adasyn = balanced_accuracy_score(y_test, y_pred_arbre_adasyn)\n",
    "specificity_arbre_adasyn = confusion_matrix(y_test, y_pred_arbre_adasyn)[0, 0] / (confusion_matrix(y_test, y_pred_arbre_adasyn)[0, 0] + confusion_matrix(y_test, y_pred_arbre_adasyn)[0, 1])\n",
    "cohen_kappa_arbre_adasyn = cohen_kappa_score(y_test, y_pred_arbre_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_arbre_bordeline_smote = roc_auc_score(y_test, y_proba_arbre_bordeline_smote)\n",
    "auc_pr_arbre_bordeline_smote = average_precision_score(y_test, y_proba_arbre_bordeline_smote)\n",
    "log_loss_value_arbre_bordeline_smote = log_loss(y_test, y_proba_arbre_bordeline_smote)\n",
    "precision_arbre_bordeline_smote = precision_score(y_test, y_pred_arbre_bordeline_smote)\n",
    "recall_arbre_bordeline_smote = recall_score(y_test, y_pred_arbre_bordeline_smote)\n",
    "f1_arbre_bordeline_smote = f1_score(y_test, y_pred_arbre_bordeline_smote)\n",
    "mcc_arbre_bordeline_smote = matthews_corrcoef(y_test, y_pred_arbre_bordeline_smote)\n",
    "balanced_acc_arbre_bordeline_smote = balanced_accuracy_score(y_test, y_pred_arbre_bordeline_smote)\n",
    "specificity_arbre_bordeline_smote = confusion_matrix(y_test, y_pred_arbre_bordeline_smote)[0, 0] / (confusion_matrix(y_test, y_pred_arbre_bordeline_smote)[0, 0] + confusion_matrix(y_test, y_pred_arbre_bordeline_smote)[0, 1])\n",
    "cohen_kappa_arbre_bordeline_smote = cohen_kappa_score(y_test, y_pred_arbre_bordeline_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "resultats_arbres = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_arbre_oversampler, auc_roc_arbre_smote, auc_roc_arbre_adasyn, auc_roc_arbre_bordeline_smote],\n",
    "    \"AUC-PR\": [auc_pr_arbre_oversampler, auc_pr_arbre_smote, auc_pr_arbre_adasyn, auc_pr_arbre_bordeline_smote],\n",
    "    \"Log Loss\": [log_loss_value_arbre_oversampler, log_loss_value_arbre_smote, log_loss_value_arbre_adasyn, log_loss_value_arbre_bordeline_smote],\n",
    "    #\"Précision\": [precision_arbre_oversampler, precision_arbre_smote, precision_arbre_adasyn, precision_arbre_bordeline_smote],\n",
    "    #\"Rappel\": [recall_arbre_oversampler, recall_arbre_smote, recall_arbre_adasyn, recall_arbre_bordeline_smote],\n",
    "    \"F1 Score\": [f1_arbre_oversampler, f1_arbre_smote, f1_arbre_adasyn, f1_arbre_bordeline_smote],\n",
    "    \"MCC\": [mcc_arbre_oversampler, mcc_arbre_smote, mcc_arbre_adasyn, mcc_arbre_bordeline_smote],\n",
    "    \"Accuracy\": [balanced_acc_arbre_oversampler, balanced_acc_arbre_smote, balanced_acc_arbre_adasyn, balanced_acc_arbre_bordeline_smote],\n",
    "    \"Spécificité\": [specificity_arbre_oversampler, specificity_arbre_smote, specificity_arbre_adasyn, specificity_arbre_bordeline_smote],\n",
    "    \"Kappa de Cohen\": [cohen_kappa_arbre_oversampler, cohen_kappa_arbre_smote, cohen_kappa_arbre_adasyn, cohen_kappa_arbre_bordeline_smote]\n",
    "}, index=[\"Arbre - RandomOverSampler\", \"Arbre - SMOTE\", \"Arbre - ADASYN\", \"Arbre - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_arbres = resultats_arbres.round(3)\n",
    "resultats_arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracé des courbes ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr_arbre_oversampler, tpr_arbre_oversampler, _ = roc_curve(y_test, y_proba_arbre_oversampler)\n",
    "fpr_arbre_smote, tpr_arbre_smote, _ = roc_curve(y_test, y_proba_arbre_smote)\n",
    "fpr_arbre_adasyn, tpr_arbre_adasyn, _ = roc_curve(y_test, y_proba_arbre_adasyn)\n",
    "fpr_arbre_bordeline_smote, tpr_arbre_bordeline_smote, _ = roc_curve(y_test, y_proba_arbre_bordeline_smote)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_arbre_oversampler, tpr_arbre_oversampler, label='Arbre avec RandomOverSampler')\n",
    "plt.plot(fpr_arbre_smote, tpr_arbre_smote, label='Arbre avec SMOTE')\n",
    "plt.plot(fpr_arbre_adasyn, tpr_arbre_adasyn, label='Arbre avec ADASYN')\n",
    "plt.plot(fpr_arbre_bordeline_smote, tpr_arbre_bordeline_smote, label='Arbre avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ajustement des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, cohen_kappa_score\n",
    "\n",
    "# Définition de la grille des hyperparamètres pour l'arbre de décision\n",
    "param_grid = {\n",
    "    'arbre__criterion': ['gini', 'entropy', 'log_loss'], # Critère d'impureté \n",
    "    'arbre__max_depth': range(3, 32, 3),  # Tester les profondeurs de 3 à 31 par pas de 3, # Profondeur maximale de l'arbre\n",
    "    'arbre__min_samples_split': [2, 5, 10, 20,30], # Nombre minimum d'échantillons pour diviser un nœud\n",
    "    'arbre__min_samples_leaf': [1, 2, 4] # Nombre minimum d'échantillons requis à chaque feuille\n",
    "}\n",
    "\n",
    "# Choix des métriques adaptées\n",
    "scoring = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Kappa' : make_scorer(cohen_kappa_score),\n",
    "    'AUC': 'roc_auc'\n",
    "}\n",
    "\n",
    "\n",
    "# Justification de l'optimisation des hyperparamètres\n",
    "\n",
    "# max_depth : pour éviter l'overfitting, on cherche à limiter la profondeur de l'arbre\n",
    "# Profondeur Minimale : 3 (pour permettre à l'arbre de commencer à capturer les interactions entre variables)\n",
    "# Profondeur Maximale : Min(2 * nombre de variables utilisables, nombre total de variables), donc dans ce cas, min(2 * 31, 31) = 31. Cela suppose que chaque variable pourrait être utilisée au maximum deux fois dans le chemin le plus long, mais cela reste une heuristique.\n",
    "# Pas : 3 ou 5 pour commencer, puis affiner en fonction des résultats obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'oversampler': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('oversampler', RandomOverSampler()), \n",
    "        ('arbre', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('smote', SMOTE()), \n",
    "        ('arbre', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'adasyn': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('adasyn', ADASYN()), \n",
    "        ('arbre', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'bordeline_smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('bordeline_smote', BorderlineSMOTE()), \n",
    "        ('arbre', DecisionTreeClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On stocke les meilleurs paramètres pour chaque modèle\n",
    "best_params = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit='F1', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    print(f\"Meilleurs paramètres pour {name} : {grid_search.best_params_}\")\n",
    "\n",
    "# On stocke les meilleurs modèles\n",
    "best_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    best_models[name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "y_proba_arbre_oversampler_cv = best_models['oversampler'].predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_oversampler_cv = best_models['oversampler'].predict(X_test)\n",
    "\n",
    "y_proba_arbre_smote_cv = best_models['smote'].predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_smote_cv = best_models['smote'].predict(X_test)\n",
    "\n",
    "y_proba_arbre_adasyn_cv = best_models['adasyn'].predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_adasyn_cv = best_models['adasyn'].predict(X_test)\n",
    "\n",
    "y_proba_arbre_bordeline_smote_cv = best_models['bordeline_smote'].predict_proba(X_test)[:, 1]\n",
    "y_pred_arbre_bordeline_smote_cv = best_models['bordeline_smote'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Mesure de la performance des modèles optimisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesure de la performance\n",
    "\n",
    "# Calcul des métriques\n",
    "auc_roc_arbre_oversampler_cv = roc_auc_score(y_test, y_proba_arbre_oversampler_cv)\n",
    "auc_pr_arbre_oversampler_cv = average_precision_score(y_test, y_proba_arbre_oversampler_cv)\n",
    "log_loss_value_arbre_oversampler_cv = log_loss(y_test, y_proba_arbre_oversampler_cv)\n",
    "precision_arbre_oversampler_cv = precision_score(y_test, y_pred_arbre_oversampler_cv)\n",
    "recall_arbre_oversampler_cv = recall_score(y_test, y_pred_arbre_oversampler_cv)\n",
    "f1_arbre_oversampler_cv = f1_score(y_test, y_pred_arbre_oversampler_cv)\n",
    "mcc_arbre_oversampler_cv = matthews_corrcoef(y_test, y_pred_arbre_oversampler_cv)\n",
    "balanced_acc_arbre_oversampler_cv = balanced_accuracy_score(y_test, y_pred_arbre_oversampler_cv)\n",
    "\n",
    "auc_roc_arbre_smote_cv = roc_auc_score(y_test, y_proba_arbre_smote_cv)\n",
    "auc_pr_arbre_smote_cv = average_precision_score(y_test, y_proba_arbre_smote_cv)\n",
    "log_loss_value_arbre_smote_cv = log_loss(y_test, y_proba_arbre_smote_cv)\n",
    "precision_arbre_smote_cv = precision_score(y_test, y_pred_arbre_smote_cv)\n",
    "recall_arbre_smote_cv = recall_score(y_test, y_pred_arbre_smote_cv)\n",
    "f1_arbre_smote_cv = f1_score(y_test, y_pred_arbre_smote_cv)\n",
    "mcc_arbre_smote_cv = matthews_corrcoef(y_test, y_pred_arbre_smote_cv)\n",
    "balanced_acc_arbre_smote_cv = balanced_accuracy_score(y_test, y_pred_arbre_smote_cv)\n",
    "\n",
    "auc_roc_arbre_adasyn_cv = roc_auc_score(y_test, y_proba_arbre_adasyn_cv)\n",
    "auc_pr_arbre_adasyn_cv = average_precision_score(y_test, y_proba_arbre_adasyn_cv)\n",
    "log_loss_value_arbre_adasyn_cv = log_loss(y_test, y_proba_arbre_adasyn_cv)\n",
    "precision_arbre_adasyn_cv = precision_score(y_test, y_pred_arbre_adasyn_cv)\n",
    "recall_arbre_adasyn_cv = recall_score(y_test, y_pred_arbre_adasyn_cv)\n",
    "f1_arbre_adasyn_cv = f1_score(y_test, y_pred_arbre_adasyn_cv)\n",
    "mcc_arbre_adasyn_cv = matthews_corrcoef(y_test, y_pred_arbre_adasyn_cv)\n",
    "balanced_acc_arbre_adasyn_cv = balanced_accuracy_score(y_test, y_pred_arbre_adasyn_cv)\n",
    "\n",
    "auc_roc_arbre_bordeline_smote_cv = roc_auc_score(y_test, y_proba_arbre_bordeline_smote_cv)\n",
    "auc_pr_arbre_bordeline_smote_cv = average_precision_score(y_test, y_proba_arbre_bordeline_smote_cv)\n",
    "log_loss_value_arbre_bordeline_smote_cv = log_loss(y_test, y_proba_arbre_bordeline_smote_cv)\n",
    "precision_arbre_bordeline_smote_cv = precision_score(y_test, y_pred_arbre_bordeline_smote_cv)\n",
    "recall_arbre_bordeline_smote_cv = recall_score(y_test, y_pred_arbre_bordeline_smote_cv)\n",
    "f1_arbre_bordeline_smote_cv = f1_score(y_test, y_pred_arbre_bordeline_smote_cv)\n",
    "mcc_arbre_bordeline_smote_cv = matthews_corrcoef(y_test, y_pred_arbre_bordeline_smote_cv)\n",
    "balanced_acc_arbre_bordeline_smote_cv = balanced_accuracy_score(y_test, y_pred_arbre_bordeline_smote_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_arbres_cv = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_arbre_oversampler_cv, auc_roc_arbre_smote_cv, auc_roc_arbre_adasyn_cv, auc_roc_arbre_bordeline_smote_cv],\n",
    "    \"AUC-PR\": [auc_pr_arbre_oversampler_cv, auc_pr_arbre_smote_cv, auc_pr_arbre_adasyn_cv, auc_pr_arbre_bordeline_smote_cv],\n",
    "    \"Log Loss\": [log_loss_value_arbre_oversampler_cv, log_loss_value_arbre_smote_cv, log_loss_value_arbre_adasyn_cv, log_loss_value_arbre_bordeline_smote_cv],\n",
    "    \"Précision\": [precision_arbre_oversampler_cv, precision_arbre_smote_cv, precision_arbre_adasyn_cv, precision_arbre_bordeline_smote_cv],\n",
    "    \"Rappel\": [recall_arbre_oversampler_cv, recall_arbre_smote_cv, recall_arbre_adasyn_cv, recall_arbre_bordeline_smote_cv],\n",
    "    \"F1 Score\": [f1_arbre_oversampler_cv, f1_arbre_smote_cv, f1_arbre_adasyn_cv, f1_arbre_bordeline_smote_cv],\n",
    "    \"MCC\": [mcc_arbre_oversampler_cv, mcc_arbre_smote_cv, mcc_arbre_adasyn_cv, mcc_arbre_bordeline_smote_cv],\n",
    "    \"Accuracy\": [balanced_acc_arbre_oversampler_cv, balanced_acc_arbre_smote_cv, balanced_acc_arbre_adasyn_cv, balanced_acc_arbre_bordeline_smote_cv]\n",
    "}, index=[\"Arbre - RandomOverSampler\", \"Arbre - SMOTE\", \"Arbre - ADASYN\", \"Arbre - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_arbres_cv = resultats_arbres_cv.round(3)\n",
    "\n",
    "resultats_arbres_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes ROC\n",
    "\n",
    "fpr_arbre_oversampler_cv, tpr_arbre_oversampler_cv, _ = roc_curve(y_test, y_proba_arbre_oversampler_cv)\n",
    "fpr_arbre_smote_cv, tpr_arbre_smote_cv, _ = roc_curve(y_test, y_proba_arbre_smote_cv)\n",
    "fpr_arbre_adasyn_cv, tpr_arbre_adasyn_cv, _ = roc_curve(y_test, y_proba_arbre_adasyn_cv)\n",
    "fpr_arbre_bordeline_smote_cv, tpr_arbre_bordeline_smote_cv, _ = roc_curve(y_test, y_proba_arbre_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_arbre_oversampler_cv, tpr_arbre_oversampler_cv, label='Arbre avec RandomOverSampler')\n",
    "plt.plot(fpr_arbre_smote_cv, tpr_arbre_smote_cv, label='Arbre avec SMOTE')\n",
    "plt.plot(fpr_arbre_adasyn_cv, tpr_arbre_adasyn_cv, label='Arbre avec ADASYN')\n",
    "plt.plot(fpr_arbre_bordeline_smote_cv, tpr_arbre_bordeline_smote_cv, label='Arbre avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "\n",
    "# Récupération de l'importance des variables pour chaque modèle\n",
    "importances_arbre_oversampler = best_models['oversampler'].named_steps['arbre'].feature_importances_\n",
    "importances_arbre_smote = best_models['smote'].named_steps['arbre'].feature_importances_\n",
    "importances_arbre_adasyn = best_models['adasyn'].named_steps['arbre'].feature_importances_\n",
    "importances_arbre_bordeline_smote = best_models['bordeline_smote'].named_steps['arbre'].feature_importances_\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "resultats_importance_variables_arbre_oversampler = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_arbre_oversampler\n",
    "})\n",
    "\n",
    "resultats_importance_variables_arbre_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_arbre_smote\n",
    "})\n",
    "\n",
    "resultats_importance_variables_arbre_adasyn = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_arbre_adasyn\n",
    "})\n",
    "\n",
    "resultats_importance_variables_arbre_bordeline_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_arbre_bordeline_smote\n",
    "})\n",
    "\n",
    "# Affichage des résultats\n",
    "resultats_importance_variables_arbre_oversampler = resultats_importance_variables_arbre_oversampler.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_arbre_smote = resultats_importance_variables_arbre_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_arbre_adasyn = resultats_importance_variables_arbre_adasyn.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_arbre_bordeline_smote = resultats_importance_variables_arbre_bordeline_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "resultats_importance_variables_arbre_oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_arbre_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_arbre_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_arbre_bordeline_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'importance des variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_arbre_oversampler['Variable'], resultats_importance_variables_arbre_oversampler['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour l\\'arbre avec RandomOverSampler')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_arbre_smote['Variable'], resultats_importance_variables_arbre_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour l\\'arbre avec SMOTE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_arbre_adasyn['Variable'], resultats_importance_variables_arbre_adasyn['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour l\\'arbre avec ADASYN')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_arbre_bordeline_smote['Variable'], resultats_importance_variables_arbre_bordeline_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour l\\'arbre avec BorderlineSMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2.4. Forêts Aléatoires "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Construction des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle de forêts aléatoires avec rééquilibrage des classes\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "etapes_rf_oversampler = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('oversampler', RandomOverSampler()), # Sur-échantillonnage\n",
    "        ('arbre', RandomForestClassifier())\n",
    "        ]\n",
    "\n",
    "etapes_rf_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('smote', SMOTE()), # Sur-échantillonnage\n",
    "        ('arbre', RandomForestClassifier()), # Forets aléatoires\n",
    "        ]\n",
    "\n",
    "etapes_rf_adasyn = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('adasyn', ADASYN()), # Sur-échantillonnage\n",
    "        ('arbre', RandomForestClassifier()), # Forets aléatoires\n",
    "        ]\n",
    "\n",
    "etapes_rf_bordeline_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('bordeline_smote', BorderlineSMOTE()), # Sur-échantillonnage\n",
    "        ('rf', RandomForestClassifier()), # Forets aléatoires\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "modele_rf_oversampler = Pipeline(steps=etapes_rf_oversampler) # Création du pipeline\n",
    "modele_rf_smote = Pipeline(steps=etapes_rf_smote) # Création du pipeline\n",
    "modele_rf_adasyn = Pipeline(steps=etapes_rf_adasyn) # Création du pipeline\n",
    "modele_rf_bordeline_smote = Pipeline(steps=etapes_rf_bordeline_smote) # Création du pipeline)\n",
    "\n",
    "modele_rf_oversampler.fit(X_train, y_train) # Entraînement\n",
    "modele_rf_smote.fit(X_train, y_train) # Entraînement\n",
    "modele_rf_adasyn.fit(X_train, y_train) # Entraînement\n",
    "modele_rf_bordeline_smote.fit(X_train, y_train) # Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_rf_oversampler = modele_rf_oversampler.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf_oversampler = modele_rf_oversampler.predict(X_test)\n",
    "\n",
    "y_proba_rf_smote = modele_rf_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf_smote = modele_rf_smote.predict(X_test)\n",
    "\n",
    "y_proba_rf_adasyn = modele_rf_adasyn.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf_adasyn = modele_rf_adasyn.predict(X_test)\n",
    "\n",
    "y_proba_rf_bordeline_smote = modele_rf_bordeline_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf_bordeline_smote = modele_rf_bordeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Mesure de la performance des modèles initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "auc_roc_rf_oversampler = roc_auc_score(y_test, y_proba_rf_oversampler)\n",
    "auc_pr_rf_oversampler = average_precision_score(y_test, y_proba_rf_oversampler)\n",
    "log_loss_value_rf_oversampler = log_loss(y_test, y_proba_rf_oversampler)\n",
    "precision_rf_oversampler = precision_score(y_test, y_pred_rf_oversampler)\n",
    "recall_rf_oversampler = recall_score(y_test, y_pred_rf_oversampler)\n",
    "f1_rf_oversampler = f1_score(y_test, y_pred_rf_oversampler)\n",
    "mcc_rf_oversampler = matthews_corrcoef(y_test, y_pred_rf_oversampler)\n",
    "balanced_acc_rf_oversampler = balanced_accuracy_score(y_test, y_pred_rf_oversampler)\n",
    "specificity_rf_oversampler = confusion_matrix(y_test, y_pred_rf_oversampler)[0, 0] / (confusion_matrix(y_test, y_pred_rf_oversampler)[0, 0] + confusion_matrix(y_test, y_pred_rf_oversampler)[0, 1])\n",
    "cohen_kappa_rf_oversampler = cohen_kappa_score(y_test, y_pred_rf_oversampler)\n",
    "\n",
    "auc_roc_rf_smote = roc_auc_score(y_test, y_proba_rf_smote)\n",
    "auc_pr_rf_smote = average_precision_score(y_test, y_proba_rf_smote)\n",
    "log_loss_value_rf_smote = log_loss(y_test, y_proba_rf_smote)\n",
    "precision_rf_smote = precision_score(y_test, y_pred_rf_smote)\n",
    "recall_rf_smote = recall_score(y_test, y_pred_rf_smote)\n",
    "f1_rf_smote = f1_score(y_test, y_pred_rf_smote)\n",
    "mcc_rf_smote = matthews_corrcoef(y_test, y_pred_rf_smote)\n",
    "balanced_acc_rf_smote = balanced_accuracy_score(y_test, y_pred_rf_smote)\n",
    "specificity_rf_smote = confusion_matrix(y_test, y_pred_rf_smote)[0, 0] / (confusion_matrix(y_test, y_pred_rf_smote)[0, 0] + confusion_matrix(y_test, y_pred_rf_smote)[0, 1])\n",
    "cohen_kappa_rf_smote = cohen_kappa_score(y_test, y_pred_rf_smote)\n",
    "\n",
    "auc_roc_rf_adasyn = roc_auc_score(y_test, y_proba_rf_adasyn)\n",
    "auc_pr_rf_adasyn = average_precision_score(y_test, y_proba_rf_adasyn)\n",
    "log_loss_value_rf_adasyn = log_loss(y_test, y_proba_rf_adasyn)\n",
    "precision_rf_adasyn = precision_score(y_test, y_pred_rf_adasyn)\n",
    "recall_rf_adasyn = recall_score(y_test, y_pred_rf_adasyn)\n",
    "f1_rf_adasyn = f1_score(y_test, y_pred_rf_adasyn)\n",
    "mcc_rf_adasyn = matthews_corrcoef(y_test, y_pred_rf_adasyn)\n",
    "balanced_acc_rf_adasyn = balanced_accuracy_score(y_test, y_pred_rf_adasyn)\n",
    "specificity_rf_adasyn = confusion_matrix(y_test, y_pred_rf_adasyn)[0, 0] / (confusion_matrix(y_test, y_pred_rf_adasyn)[0, 0] + confusion_matrix(y_test, y_pred_rf_adasyn)[0, 1])\n",
    "cohen_kappa_rf_adasyn = cohen_kappa_score(y_test, y_pred_rf_adasyn)\n",
    "\n",
    "auc_roc_rf_bordeline_smote = roc_auc_score(y_test, y_proba_rf_bordeline_smote)\n",
    "auc_pr_rf_bordeline_smote = average_precision_score(y_test, y_proba_rf_bordeline_smote)\n",
    "log_loss_value_rf_bordeline_smote = log_loss(y_test, y_proba_rf_bordeline_smote)\n",
    "precision_rf_bordeline_smote = precision_score(y_test, y_pred_rf_bordeline_smote)\n",
    "recall_rf_bordeline_smote = recall_score(y_test, y_pred_rf_bordeline_smote)\n",
    "f1_rf_bordeline_smote = f1_score(y_test, y_pred_rf_bordeline_smote)\n",
    "mcc_rf_bordeline_smote = matthews_corrcoef(y_test, y_pred_rf_bordeline_smote)\n",
    "balanced_acc_rf_bordeline_smote = balanced_accuracy_score(y_test, y_pred_rf_bordeline_smote)\n",
    "specificity_rf_bordeline_smote = confusion_matrix(y_test, y_pred_rf_bordeline_smote)[0, 0] / (confusion_matrix(y_test, y_pred_rf_bordeline_smote)[0, 0] + confusion_matrix(y_test, y_pred_rf_bordeline_smote)[0, 1])\n",
    "cohen_kappa_rf_bordeline_smote = cohen_kappa_score(y_test, y_pred_rf_bordeline_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_rf = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_rf_oversampler, auc_roc_rf_smote, auc_roc_rf_adasyn, auc_roc_rf_bordeline_smote],\n",
    "    \"AUC-PR\": [auc_pr_rf_oversampler, auc_pr_rf_smote, auc_pr_rf_adasyn, auc_pr_rf_bordeline_smote],\n",
    "    \"Log Loss\": [log_loss_value_rf_oversampler, log_loss_value_rf_smote, log_loss_value_rf_adasyn, log_loss_value_rf_bordeline_smote],\n",
    "    #\"Précision\": [precision_rf_oversampler, precision_rf_smote, precision_rf_adasyn, precision_rf_bordeline_smote],\n",
    "    #\"Rappel\": [recall_rf_oversampler, recall_rf_smote, recall_rf_adasyn, recall_rf_bordeline_smote],\n",
    "    \"F1 Score\": [f1_rf_oversampler, f1_rf_smote, f1_rf_adasyn, f1_rf_bordeline_smote],\n",
    "    \"MCC\": [mcc_rf_oversampler, mcc_rf_smote, mcc_rf_adasyn, mcc_rf_bordeline_smote],\n",
    "    \"Accuracy\": [balanced_acc_rf_oversampler, balanced_acc_rf_smote, balanced_acc_rf_adasyn, balanced_acc_rf_bordeline_smote],\n",
    "    \"Spécificité\": [specificity_rf_oversampler, specificity_rf_smote, specificity_rf_adasyn, specificity_rf_bordeline_smote],\n",
    "    \"Kappa de Cohen\": [cohen_kappa_rf_oversampler, cohen_kappa_rf_smote, cohen_kappa_rf_adasyn, cohen_kappa_rf_bordeline_smote]\n",
    "}, index=[\"RF - RandomOverSampler\", \"RF - SMOTE\", \"RF - ADASYN\", \"RF - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_rf = resultats_rf.round(3)\n",
    "resultats_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_rf_oversampler, tpr_rf_oversampler, _ = roc_curve(y_test, y_proba_rf_oversampler)\n",
    "fpr_rf_smote, tpr_rf_smote, _ = roc_curve(y_test, y_proba_rf_smote)\n",
    "fpr_rf_adasyn, tpr_rf_adasyn, _ = roc_curve(y_test, y_proba_rf_adasyn)\n",
    "fpr_rf_bordeline_smote, tpr_rf_bordeline_smote, _ = roc_curve(y_test, y_proba_rf_bordeline_smote)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_rf_oversampler, tpr_rf_oversampler, label='RF avec RandomOverSampler')\n",
    "plt.plot(fpr_rf_smote, tpr_rf_smote, label='RF avec SMOTE')\n",
    "plt.plot(fpr_rf_adasyn, tpr_rf_adasyn, label='RF avec ADASYN')\n",
    "plt.plot(fpr_rf_bordeline_smote, tpr_rf_bordeline_smote, label='RF avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ajustement des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement des hyperparamètres par validation croisée\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "# Définition de la grille des hyperparamètres pour les forêts aléatoires\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300, 400, 500], # Nombre d'arbres\n",
    "    'rf__criterion': ['gini', 'entropy'], # Critère d'impureté\n",
    "    'rf__max_depth': range(3,32,3), # Profondeur maximale de l'arbre\n",
    "    'rf__min_samples_split': [2, 5, 10, 20, 30], # Nombre minimum d'échantillons pour diviser un nœud\n",
    "    'rf__min_samples_leaf': [1, 2, 4] # Nombre minimum d'échantillons requis à chaque feuille\n",
    "}\n",
    "\n",
    "# Choix des métriques adaptées\n",
    "scoring = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Kappa': make_scorer(cohen_kappa_score),\n",
    "    'AUC': 'roc_auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "\n",
    "pipelines = {\n",
    "    'oversampler': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('oversampler', RandomOverSampler()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('smote', SMOTE()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'adasyn': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('adasyn', ADASYN()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'bordeline_smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('bordeline_smote', BorderlineSMOTE()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On stocke les meilleurs paramètres pour chaque modèle\n",
    "best_params = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit='F1', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    print(f\"Meilleurs paramètres pour {name} : {grid_search.best_params_}\")\n",
    "\n",
    "# On stocke les meilleurs modèles\n",
    "best_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    best_models[name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_rf_oversampler_cv = best_models['oversampler'].predict_proba(X_test)[:, 1]\n",
    "y_proba_rf_smote_cv = best_models['smote'].predict_proba(X_test)[:, 1]\n",
    "y_proba_rf_adasyn_cv = best_models['adasyn'].predict_proba(X_test)[:, 1]\n",
    "y_proba_rf_bordeline_smote_cv = best_models['bordeline_smote'].predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Mesure de la performance des modèles optimisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "auc_roc_rf_oversampler_cv = roc_auc_score(y_test, y_proba_rf_oversampler_cv)\n",
    "auc_pr_rf_oversampler_cv = average_precision_score(y_test, y_proba_rf_oversampler_cv)\n",
    "log_loss_value_rf_oversampler_cv = log_loss(y_test, y_proba_rf_oversampler_cv)\n",
    "precision_rf_oversampler_cv = precision_score(y_test, y_proba_rf_oversampler_cv)\n",
    "recall_rf_oversampler_cv = recall_score(y_test, y_proba_rf_oversampler_cv)\n",
    "f1_rf_oversampler_cv = f1_score(y_test, y_proba_rf_oversampler_cv)\n",
    "mcc_rf_oversampler_cv = matthews_corrcoef(y_test, y_proba_rf_oversampler_cv)\n",
    "balanced_acc_rf_oversampler_cv = balanced_accuracy_score(y_test, y_proba_rf_oversampler_cv)\n",
    "\n",
    "auc_roc_rf_smote_cv = roc_auc_score(y_test, y_proba_rf_smote_cv)\n",
    "auc_pr_rf_smote_cv = average_precision_score(y_test, y_proba_rf_smote_cv)\n",
    "log_loss_value_rf_smote_cv = log_loss(y_test, y_proba_rf_smote_cv)\n",
    "precision_rf_smote_cv = precision_score(y_test, y_proba_rf_smote_cv)\n",
    "recall_rf_smote_cv = recall_score(y_test, y_proba_rf_smote_cv)\n",
    "f1_rf_smote_cv = f1_score(y_test, y_proba_rf_smote_cv)\n",
    "mcc_rf_smote_cv = matthews_corrcoef(y_test, y_proba_rf_smote_cv)\n",
    "balanced_acc_rf_smote_cv = balanced_accuracy_score(y_test, y_proba_rf_smote_cv)\n",
    "\n",
    "auc_roc_rf_adasyn_cv = roc_auc_score(y_test, y_proba_rf_adasyn_cv)\n",
    "auc_pr_rf_adasyn_cv = average_precision_score(y_test, y_proba_rf_adasyn_cv)\n",
    "log_loss_value_rf_adasyn_cv = log_loss(y_test, y_proba_rf_adasyn_cv)\n",
    "precision_rf_adasyn_cv = precision_score(y_test, y_proba_rf_adasyn_cv)\n",
    "recall_rf_adasyn_cv = recall_score(y_test, y_proba_rf_adasyn_cv)\n",
    "f1_rf_adasyn_cv = f1_score(y_test, y_proba_rf_adasyn_cv)\n",
    "mcc_rf_adasyn_cv = matthews_corrcoef(y_test, y_proba_rf_adasyn_cv)\n",
    "balanced_acc_rf_adasyn_cv = balanced_accuracy_score(y_test, y_proba_rf_adasyn_cv)\n",
    "\n",
    "auc_roc_rf_bordeline_smote_cv = roc_auc_score(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "auc_pr_rf_bordeline_smote_cv = average_precision_score(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "log_loss_value_rf_bordeline_smote_cv = log_loss(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "precision_rf_bordeline_smote_cv = precision_score(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "recall_rf_bordeline_smote_cv = recall_score(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "f1_rf_bordeline_smote_cv = f1_score(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "mcc_rf_bordeline_smote_cv = matthews_corrcoef(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "balanced_acc_rf_bordeline_smote_cv = balanced_accuracy_score(y_test, y_proba_rf_bordeline_smote_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_rf_cv = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_rf_oversampler_cv, auc_roc_rf_smote_cv, auc_roc_rf_adasyn_cv, auc_roc_rf_bordeline_smote_cv],\n",
    "    \"AUC-PR\": [auc_pr_rf_oversampler_cv, auc_pr_rf_smote_cv, auc_pr_rf_adasyn_cv, auc_pr_rf_bordeline_smote_cv],\n",
    "    \"Log Loss\": [log_loss_value_rf_oversampler_cv, log_loss_value_rf_smote_cv, log_loss_value_rf_adasyn_cv, log_loss_value_rf_bordeline_smote_cv],\n",
    "    \"Précision\": [precision_rf_oversampler_cv, precision_rf_smote_cv, precision_rf_adasyn_cv, precision_rf_bordeline_smote_cv],\n",
    "    \"Rappel\": [recall_rf_oversampler_cv, recall_rf_smote_cv, recall_rf_adasyn_cv, recall_rf_bordeline_smote_cv],\n",
    "    \"F1 Score\": [f1_rf_oversampler_cv, f1_rf_smote_cv, f1_rf_adasyn_cv, f1_rf_bordeline_smote_cv],\n",
    "    \"MCC\": [mcc_rf_oversampler_cv, mcc_rf_smote_cv, mcc_rf_adasyn_cv, mcc_rf_bordeline_smote_cv],\n",
    "    \"Accuracy\": [balanced_acc_rf_oversampler_cv, balanced_acc_rf_smote_cv, balanced_acc_rf_adasyn_cv, balanced_acc_rf_bordeline_smote_cv]\n",
    "}, index=[\"RF - RandomOverSampler\", \"RF - SMOTE\", \"RF - ADASYN\", \"RF - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_rf_cv = resultats_rf_cv.round(3)\n",
    "resultats_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes ROC\n",
    "\n",
    "fpr_rf_oversampler_cv, tpr_rf_oversampler_cv, _ = roc_curve(y_test, y_proba_rf_oversampler_cv)\n",
    "fpr_rf_smote_cv, tpr_rf_smote_cv, _ = roc_curve(y_test, y_proba_rf_smote_cv)\n",
    "fpr_rf_adasyn_cv, tpr_rf_adasyn_cv, _ = roc_curve(y_test, y_proba_rf_adasyn_cv)\n",
    "fpr_rf_bordeline_smote_cv, tpr_rf_bordeline_smote_cv, _ = roc_curve(y_test, y_proba_rf_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_rf_oversampler_cv, tpr_rf_oversampler_cv, label='RF avec RandomOverSampler')\n",
    "plt.plot(fpr_rf_smote_cv, tpr_rf_smote_cv, label='RF avec SMOTE')\n",
    "plt.plot(fpr_rf_adasyn_cv, tpr_rf_adasyn_cv, label='RF avec ADASYN')\n",
    "plt.plot(fpr_rf_bordeline_smote_cv, tpr_rf_bordeline_smote_cv, label='RF avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "\n",
    "# Récupération de l'importance des variables pour chaque modèle\n",
    "importances_rf_oversampler = best_models['oversampler'].named_steps['rf'].feature_importances_\n",
    "importances_rf_smote = best_models['smote'].named_steps['rf'].feature_importances_\n",
    "importances_rf_adasyn = best_models['adasyn'].named_steps['rf'].feature_importances_\n",
    "importances_rf_bordeline_smote = best_models['bordeline_smote'].named_steps['rf'].feature_importances_\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "resultats_importance_variables_rf_oversampler = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_rf_oversampler\n",
    "})\n",
    "\n",
    "resultats_importance_variables_rf_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_rf_smote\n",
    "})\n",
    "\n",
    "resultats_importance_variables_rf_adasyn = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_rf_adasyn\n",
    "})\n",
    "\n",
    "resultats_importance_variables_rf_bordeline_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_rf_bordeline_smote\n",
    "})\n",
    "\n",
    "# Affichage des résultats\n",
    "resultats_importance_variables_rf_oversampler = resultats_importance_variables_rf_oversampler.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_rf_smote = resultats_importance_variables_rf_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_rf_adasyn = resultats_importance_variables_rf_adasyn.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_rf_bordeline_smote = resultats_importance_variables_rf_bordeline_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "resultats_importance_variables_rf_oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_rf_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_rf_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_rf_bordeline_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'importance des variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_rf_oversampler['Variable'], resultats_importance_variables_rf_oversampler['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le RF avec RandomOverSampler')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_rf_smote['Variable'], resultats_importance_variables_rf_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le RF avec SMOTE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_rf_adasyn['Variable'], resultats_importance_variables_rf_adasyn['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le RF avec ADASYN')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_rf_bordeline_smote['Variable'], resultats_importance_variables_rf_bordeline_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le RF avec BorderlineSMOTE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2.4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Construction des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Construction des étapes\n",
    "etapes_gb_randomoversampler = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('oversampler', RandomOverSampler()), # Sur-échantillonnage\n",
    "        ('gb', GradientBoostingClassifier()) # Gradient Boosting\n",
    "        ]\n",
    "\n",
    "etapes_gb_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('smote', SMOTE()), # Sur-échantillonnage\n",
    "        ('gb', GradientBoostingClassifier()) # Gradient Boosting\n",
    "        ]\n",
    "\n",
    "etapes_gb_adasyn = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('adasyn', ADASYN()), # Sur-échantillonnage\n",
    "        ('gb', GradientBoostingClassifier()) # Gradient Boosting\n",
    "        ]\n",
    "\n",
    "etapes_gb_bordeline_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('bordeline_smote', BorderlineSMOTE()), # Sur-échantillonnage\n",
    "        ('gb', GradientBoostingClassifier()) # Gradient Boosting\n",
    "        ]\n",
    "\n",
    "modele_gb_randomoversampler = Pipeline(steps=etapes_gb_randomoversampler) # Création du pipeline\n",
    "modele_gb_smote = Pipeline(steps=etapes_gb_smote) # Création du pipeline\n",
    "modele_gb_adasyn = Pipeline(steps=etapes_gb_adasyn) # Création du pipeline\n",
    "modele_gb_bordeline_smote = Pipeline(steps=etapes_gb_bordeline_smote) # Création du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement des modèles\n",
    "modele_gb_randomoversampler.fit(X_train, y_train) # Entraînement\n",
    "modele_gb_smote.fit(X_train, y_train) # Entraînement\n",
    "modele_gb_adasyn.fit(X_train, y_train) # Entraînement\n",
    "modele_gb_bordeline_smote.fit(X_train, y_train) # Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_gb_randomoversampler = modele_gb_randomoversampler.predict_proba(X_test)[:, 1]\n",
    "y_pred_gb_randomoversampler = modele_gb_randomoversampler.predict(X_test)\n",
    "\n",
    "y_proba_gb_smote = modele_gb_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_gb_smote = modele_gb_smote.predict(X_test)\n",
    "\n",
    "y_proba_gb_adasyn = modele_gb_adasyn.predict_proba(X_test)[:, 1]\n",
    "y_pred_gb_adasyn = modele_gb_adasyn.predict(X_test)\n",
    "\n",
    "y_proba_gb_bordeline_smote = modele_gb_bordeline_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_gb_bordeline_smote = modele_gb_bordeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Mesure de la performance des modèles initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "auc_roc_gb_randomoversampler = roc_auc_score(y_test, y_proba_gb_randomoversampler)\n",
    "auc_pr_gb_randomoversampler = average_precision_score(y_test, y_proba_gb_randomoversampler)\n",
    "log_loss_value_gb_randomoversampler = log_loss(y_test, y_proba_gb_randomoversampler)\n",
    "precision_gb_randomoversampler = precision_score(y_test, y_pred_gb_randomoversampler)\n",
    "recall_gb_randomoversampler = recall_score(y_test, y_pred_gb_randomoversampler)\n",
    "f1_gb_randomoversampler = f1_score(y_test, y_pred_gb_randomoversampler)\n",
    "mcc_gb_randomoversampler = matthews_corrcoef(y_test, y_pred_gb_randomoversampler)\n",
    "balanced_acc_gb_randomoversampler = balanced_accuracy_score(y_test, y_pred_gb_randomoversampler)\n",
    "specificity_gb_randomoversampler = confusion_matrix(y_test, y_pred_gb_randomoversampler)[0, 0] / (confusion_matrix(y_test, y_pred_gb_randomoversampler)[0, 0] + confusion_matrix(y_test, y_pred_gb_randomoversampler)[0, 1])\n",
    "cohen_kappa_gb_randomoversampler = cohen_kappa_score(y_test, y_pred_gb_randomoversampler)\n",
    "\n",
    "auc_roc_gb_smote = roc_auc_score(y_test, y_proba_gb_smote)\n",
    "auc_pr_gb_smote = average_precision_score(y_test, y_proba_gb_smote)\n",
    "log_loss_value_gb_smote = log_loss(y_test, y_proba_gb_smote)\n",
    "precision_gb_smote = precision_score(y_test, y_pred_gb_smote)\n",
    "recall_gb_smote = recall_score(y_test, y_pred_gb_smote)\n",
    "f1_gb_smote = f1_score(y_test, y_pred_gb_smote)\n",
    "mcc_gb_smote = matthews_corrcoef(y_test, y_pred_gb_smote)\n",
    "balanced_acc_gb_smote = balanced_accuracy_score(y_test, y_pred_gb_smote)\n",
    "specificity_gb_smote = confusion_matrix(y_test, y_pred_gb_smote)[0, 0] / (confusion_matrix(y_test, y_pred_gb_smote)[0, 0] + confusion_matrix(y_test, y_pred_gb_smote)[0, 1])\n",
    "cohen_kappa_gb_smote = cohen_kappa_score(y_test, y_pred_gb_smote)\n",
    "\n",
    "auc_roc_gb_adasyn = roc_auc_score(y_test, y_proba_gb_adasyn)\n",
    "auc_pr_gb_adasyn = average_precision_score(y_test, y_proba_gb_adasyn)\n",
    "log_loss_value_gb_adasyn = log_loss(y_test, y_proba_gb_adasyn)\n",
    "precision_gb_adasyn = precision_score(y_test, y_pred_gb_adasyn)\n",
    "recall_gb_adasyn = recall_score(y_test, y_pred_gb_adasyn)\n",
    "f1_gb_adasyn = f1_score(y_test, y_pred_gb_adasyn)\n",
    "mcc_gb_adasyn = matthews_corrcoef(y_test, y_pred_gb_adasyn)\n",
    "balanced_acc_gb_adasyn = balanced_accuracy_score(y_test, y_pred_gb_adasyn)\n",
    "specificity_gb_adasyn = confusion_matrix(y_test, y_pred_gb_adasyn)[0, 0] / (confusion_matrix(y_test, y_pred_gb_adasyn)[0, 0] + confusion_matrix(y_test, y_pred_gb_adasyn)[0, 1])\n",
    "cohen_kappa_gb_adasyn = cohen_kappa_score(y_test, y_pred_gb_adasyn)\n",
    "\n",
    "auc_roc_gb_bordeline_smote = roc_auc_score(y_test, y_proba_gb_bordeline_smote)\n",
    "auc_pr_gb_bordeline_smote = average_precision_score(y_test, y_proba_gb_bordeline_smote)\n",
    "log_loss_value_gb_bordeline_smote = log_loss(y_test, y_proba_gb_bordeline_smote)\n",
    "precision_gb_bordeline_smote = precision_score(y_test, y_pred_gb_bordeline_smote)\n",
    "recall_gb_bordeline_smote = recall_score(y_test, y_pred_gb_bordeline_smote)\n",
    "f1_gb_bordeline_smote = f1_score(y_test, y_pred_gb_bordeline_smote)\n",
    "mcc_gb_bordeline_smote = matthews_corrcoef(y_test, y_pred_gb_bordeline_smote)\n",
    "balanced_acc_gb_bordeline_smote = balanced_accuracy_score(y_test, y_pred_gb_bordeline_smote)\n",
    "specificity_gb_bordeline_smote = confusion_matrix(y_test, y_pred_gb_bordeline_smote)[0, 0] / (confusion_matrix(y_test, y_pred_gb_bordeline_smote)[0, 0] + confusion_matrix(y_test, y_pred_gb_bordeline_smote)[0, 1])\n",
    "cohen_kappa_gb_bordeline_smote = cohen_kappa_score(y_test, y_pred_gb_bordeline_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "resultats_gb = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_gb_randomoversampler, auc_roc_gb_smote, auc_roc_gb_adasyn, auc_roc_gb_bordeline_smote],\n",
    "    \"AUC-PR\": [auc_pr_gb_randomoversampler, auc_pr_gb_smote, auc_pr_gb_adasyn, auc_pr_gb_bordeline_smote],\n",
    "    \"Log Loss\": [log_loss_value_gb_randomoversampler, log_loss_value_gb_smote, log_loss_value_gb_adasyn, log_loss_value_gb_bordeline_smote],\n",
    "    #\"Précision\": [precision_gb_randomoversampler, precision_gb_smote, precision_gb_adasyn, precision_gb_bordeline_smote],\n",
    "    #\"Rappel\": [recall_gb_randomoversampler, recall_gb_smote, recall_gb_adasyn, recall_gb_bordeline_smote],\n",
    "    \"F1 Score\": [f1_gb_randomoversampler, f1_gb_smote, f1_gb_adasyn, f1_gb_bordeline_smote],\n",
    "    \"MCC\": [mcc_gb_randomoversampler, mcc_gb_smote, mcc_gb_adasyn, mcc_gb_bordeline_smote],\n",
    "    \"Accuracy\": [balanced_acc_gb_randomoversampler, balanced_acc_gb_smote, balanced_acc_gb_adasyn, balanced_acc_gb_bordeline_smote],\n",
    "    \"Spécificité\": [specificity_gb_randomoversampler, specificity_gb_smote, specificity_gb_adasyn, specificity_gb_bordeline_smote],\n",
    "    \"Kappa de Cohen\": [cohen_kappa_gb_randomoversampler, cohen_kappa_gb_smote, cohen_kappa_gb_adasyn, cohen_kappa_gb_bordeline_smote]\n",
    "}, index=[\"GB - RandomOverSampler\", \"GB - SMOTE\", \"GB - ADASYN\", \"GB - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_gb = resultats_gb.round(3)\n",
    "resultats_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_gb_randomoversampler, tpr_gb_randomoversampler, _ = roc_curve(y_test, y_proba_gb_randomoversampler)\n",
    "fpr_gb_smote, tpr_gb_smote, _ = roc_curve(y_test, y_proba_gb_smote)\n",
    "fpr_gb_adasyn, tpr_gb_adasyn, _ = roc_curve(y_test, y_proba_gb_adasyn)\n",
    "fpr_gb_bordeline_smote, tpr_gb_bordeline_smote, _ = roc_curve(y_test, y_proba_gb_bordeline_smote)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_gb_randomoversampler, tpr_gb_randomoversampler, label='GB avec RandomOverSampler')\n",
    "plt.plot(fpr_gb_smote, tpr_gb_smote, label='GB avec SMOTE')\n",
    "plt.plot(fpr_gb_adasyn, tpr_gb_adasyn, label='GB avec ADASYN')\n",
    "plt.plot(fpr_gb_bordeline_smote, tpr_gb_bordeline_smote, label='GB avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ajustement des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement des hyperparamètres par validation croisée\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définition de la grille des hyperparamètres pour le gradient boosting\n",
    "\n",
    "param_grid = {\n",
    "    'gb__n_estimators': [100, 200, 300, 400, 500], # Nombre d'arbres\n",
    "    'gb__learning_rate': [0.01, 0.05, 0.1, 0.5], # Taux d'apprentissage\n",
    "    'gb__max_depth': [3, 5, 7, 9], # Profondeur maximale de l'arbre\n",
    "}\n",
    "\n",
    "# Choix des métriques adaptées\n",
    "scoring = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Recall': make_scorer(recall_score),\n",
    "    'AUC': 'roc_auc'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "\n",
    "pipelines = {\n",
    "    'oversampler': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('oversampler', RandomOverSampler()), \n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('smote', SMOTE()), \n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'adasyn': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('adasyn', ADASYN()), \n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'bordeline_smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('bordeline_smote', BorderlineSMOTE()), \n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On stocke les meilleurs paramètres pour chaque modèle\n",
    "best_params = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit='F1', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    print(f\"Meilleurs paramètres pour {name} : {grid_search.best_params_}\")\n",
    "\n",
    "# On stocke les meilleurs modèles\n",
    "best_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    best_models[name] = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_gb_randomoversampler_cv = best_models['oversampler'].predict_proba(X_test)[:, 1]\n",
    "y_proba_gb_smote_cv = best_models['smote'].predict_proba(X_test)[:, 1]\n",
    "y_proba_gb_adasyn_cv = best_models['adasyn'].predict_proba(X_test)[:, 1]\n",
    "y_proba_gb_bordeline_smote_cv = best_models['bordeline_smote'].predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Mesure de la performance des modèles optimisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "auc_roc_gb_randomoversampler_cv = roc_auc_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "auc_pr_gb_randomoversampler_cv = average_precision_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "log_loss_value_gb_randomoversampler_cv = log_loss(y_test, y_proba_gb_randomoversampler_cv)\n",
    "precision_gb_randomoversampler_cv = precision_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "recall_gb_randomoversampler_cv = recall_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "f1_gb_randomoversampler_cv = f1_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "mcc_gb_randomoversampler_cv = matthews_corrcoef(y_test, y_proba_gb_randomoversampler_cv)\n",
    "balanced_acc_gb_randomoversampler_cv = balanced_accuracy_score(y_test, y_proba_gb_randomoversampler_cv)\n",
    "\n",
    "auc_roc_gb_smote_cv = roc_auc_score(y_test, y_proba_gb_smote_cv)\n",
    "auc_pr_gb_smote_cv = average_precision_score(y_test, y_proba_gb_smote_cv)\n",
    "log_loss_value_gb_smote_cv = log_loss(y_test, y_proba_gb_smote_cv)\n",
    "precision_gb_smote_cv = precision_score(y_test, y_proba_gb_smote_cv)\n",
    "recall_gb_smote_cv = recall_score(y_test, y_proba_gb_smote_cv)\n",
    "f1_gb_smote_cv = f1_score(y_test, y_proba_gb_smote_cv)\n",
    "mcc_gb_smote_cv = matthews_corrcoef(y_test, y_proba_gb_smote_cv)\n",
    "balanced_acc_gb_smote_cv = balanced_accuracy_score(y_test, y_proba_gb_smote_cv)\n",
    "\n",
    "auc_roc_gb_adasyn_cv = roc_auc_score(y_test, y_proba_gb_adasyn_cv)\n",
    "auc_pr_gb_adasyn_cv = average_precision_score(y_test, y_proba_gb_adasyn_cv)\n",
    "log_loss_value_gb_adasyn_cv = log_loss(y_test, y_proba_gb_adasyn_cv)\n",
    "precision_gb_adasyn_cv = precision_score(y_test, y_proba_gb_adasyn_cv)\n",
    "recall_gb_adasyn_cv = recall_score(y_test, y_proba_gb_adasyn_cv)\n",
    "f1_gb_adasyn_cv = f1_score(y_test, y_proba_gb_adasyn_cv)\n",
    "mcc_gb_adasyn_cv = matthews_corrcoef(y_test, y_proba_gb_adasyn_cv)\n",
    "balanced_acc_gb_adasyn_cv = balanced_accuracy_score(y_test, y_proba_gb_adasyn_cv)\n",
    "\n",
    "auc_roc_gb_bordeline_smote_cv = roc_auc_score(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "auc_pr_gb_bordeline_smote_cv = average_precision_score(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "log_loss_value_gb_bordeline_smote_cv = log_loss(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "precision_gb_bordeline_smote_cv = precision_score(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "recall_gb_bordeline_smote_cv = recall_score(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "f1_gb_bordeline_smote_cv = f1_score(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "mcc_gb_bordeline_smote_cv = matthews_corrcoef(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "balanced_acc_gb_bordeline_smote_cv = balanced_accuracy_score(y_test, y_proba_gb_bordeline_smote_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_gb_cv = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_gb_randomoversampler_cv, auc_roc_gb_smote_cv, auc_roc_gb_adasyn_cv, auc_roc_gb_bordeline_smote_cv],\n",
    "    \"AUC-PR\": [auc_pr_gb_randomoversampler_cv, auc_pr_gb_smote_cv, auc_pr_gb_adasyn_cv, auc_pr_gb_bordeline_smote_cv],\n",
    "    \"Log Loss\": [log_loss_value_gb_randomoversampler_cv, log_loss_value_gb_smote_cv, log_loss_value_gb_adasyn_cv, log_loss_value_gb_bordeline_smote_cv],\n",
    "    \"Précision\": [precision_gb_randomoversampler_cv, precision_gb_smote_cv, precision_gb_adasyn_cv, precision_gb_bordeline_smote_cv],\n",
    "    \"Rappel\": [recall_gb_randomoversampler_cv, recall_gb_smote_cv, recall_gb_adasyn_cv, recall_gb_bordeline_smote_cv],\n",
    "    \"F1 Score\": [f1_gb_randomoversampler_cv, f1_gb_smote_cv, f1_gb_adasyn_cv, f1_gb_bordeline_smote_cv],\n",
    "    \"MCC\": [mcc_gb_randomoversampler_cv, mcc_gb_smote_cv, mcc_gb_adasyn_cv, mcc_gb_bordeline_smote_cv],\n",
    "    \"Accuracy\": [balanced_acc_gb_randomoversampler_cv, balanced_acc_gb_smote_cv, balanced_acc_gb_adasyn_cv, balanced_acc_gb_bordeline_smote_cv]\n",
    "}, index=[\"GB - RandomOverSampler\", \"GB - SMOTE\", \"GB - ADASYN\", \"GB - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_gb_cv = resultats_gb_cv.round(3)\n",
    "resultats_gb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "fpr_gb_randomoversampler_cv, tpr_gb_randomoversampler_cv, _ = roc_curve(y_test, y_proba_gb_randomoversampler_cv)\n",
    "fpr_gb_smote_cv, tpr_gb_smote_cv, _ = roc_curve(y_test, y_proba_gb_smote_cv)\n",
    "fpr_gb_adasyn_cv, tpr_gb_adasyn_cv, _ = roc_curve(y_test, y_proba_gb_adasyn_cv)\n",
    "fpr_gb_bordeline_smote_cv, tpr_gb_bordeline_smote_cv, _ = roc_curve(y_test, y_proba_gb_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_gb_randomoversampler_cv, tpr_gb_randomoversampler_cv, label='GB avec RandomOverSampler')\n",
    "plt.plot(fpr_gb_smote_cv, tpr_gb_smote_cv, label='GB avec SMOTE')\n",
    "plt.plot(fpr_gb_adasyn_cv, tpr_gb_adasyn_cv, label='GB avec ADASYN')\n",
    "plt.plot(fpr_gb_bordeline_smote_cv, tpr_gb_bordeline_smote_cv, label='GB avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "\n",
    "# Récupération de l'importance des variables pour chaque modèle\n",
    "importances_gb_randomoversampler = best_models['oversampler'].named_steps['gb'].feature_importances_\n",
    "importances_gb_smote = best_models['smote'].named_steps['gb'].feature_importances_\n",
    "importances_gb_adasyn = best_models['adasyn'].named_steps['gb'].feature_importances_\n",
    "importances_gb_bordeline_smote = best_models['bordeline_smote'].named_steps['gb'].feature_importances_\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "resultats_importance_variables_gb_randomoversampler = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_gb_randomoversampler\n",
    "})\n",
    "\n",
    "resultats_importance_variables_gb_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_gb_smote\n",
    "})\n",
    "\n",
    "resultats_importance_variables_gb_adasyn = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_gb_adasyn\n",
    "})\n",
    "\n",
    "resultats_importance_variables_gb_bordeline_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_gb_bordeline_smote\n",
    "})\n",
    "\n",
    "# Affichage des résultats\n",
    "resultats_importance_variables_gb_randomoversampler = resultats_importance_variables_gb_randomoversampler.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_gb_smote = resultats_importance_variables_gb_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_gb_adasyn = resultats_importance_variables_gb_adasyn.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_gb_bordeline_smote = resultats_importance_variables_gb_bordeline_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "resultats_importance_variables_gb_randomoversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_gb_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_gb_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_gb_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'importance des variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_gb_randomoversampler['Variable'], resultats_importance_variables_gb_randomoversampler['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le GB avec RandomOverSampler')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_gb_smote['Variable'], resultats_importance_variables_gb_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le GB avec SMOTE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_gb_adasyn['Variable'], resultats_importance_variables_gb_adasyn['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le GB avec ADASYN')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_gb_bordeline_smote['Variable'], resultats_importance_variables_gb_bordeline_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le GB avec BorderlineSMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2.5. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Construction des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction des modèles\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "etapes_adaboost_randomoversampler = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('oversampler', RandomOverSampler()), # Sur-échantillonnage\n",
    "        ('adaboost', AdaBoostClassifier()) # AdaBoost\n",
    "        ]\n",
    "\n",
    "etapes_adaboost_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('smote', SMOTE()), # Sur-échantillonnage\n",
    "        ('adaboost', AdaBoostClassifier()) # AdaBoost\n",
    "        ]\n",
    "\n",
    "etapes_adaboost_adasyn = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('adasyn', ADASYN()), # Sur-échantillonnage\n",
    "        ('adaboost', AdaBoostClassifier()) # AdaBoost\n",
    "        ]\n",
    "\n",
    "etapes_adaboost_bordeline_smote = [('std_scaler', StandardScaler()), # Standardisation des variables\n",
    "        ('bordeline_smote', BorderlineSMOTE()), # Sur-échantillonnage\n",
    "        ('adaboost', AdaBoostClassifier()) # AdaBoost\n",
    "        ]\n",
    "\n",
    "modele_adaboost_randomoversampler = Pipeline(steps=etapes_adaboost_randomoversampler) # Création du pipeline\n",
    "modele_adaboost_smote = Pipeline(steps=etapes_adaboost_smote) # Création du pipeline\n",
    "modele_adaboost_adasyn = Pipeline(steps=etapes_adaboost_adasyn) # Création du pipeline\n",
    "modele_adaboost_bordeline_smote = Pipeline(steps=etapes_adaboost_bordeline_smote) # Création du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_adaboost_randomoversampler.fit(X_train, y_train) # Entraînement\n",
    "modele_adaboost_smote.fit(X_train, y_train) # Entraînement\n",
    "modele_adaboost_adasyn.fit(X_train, y_train) # Entraînement\n",
    "modele_adaboost_bordeline_smote.fit(X_train, y_train) # Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_adaboost_randomoversampler = modele_adaboost_randomoversampler.predict_proba(X_test)[:, 1]\n",
    "y_pred_adaboost_randomoversampler = modele_adaboost_randomoversampler.predict(X_test)\n",
    "\n",
    "y_proba_adaboost_smote = modele_adaboost_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_adaboost_smote = modele_adaboost_smote.predict(X_test)\n",
    "\n",
    "y_proba_adaboost_adasyn = modele_adaboost_adasyn.predict_proba(X_test)[:, 1]\n",
    "y_pred_adaboost_adasyn = modele_adaboost_adasyn.predict(X_test)\n",
    "\n",
    "y_proba_adaboost_bordeline_smote = modele_adaboost_bordeline_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_adaboost_bordeline_smote = modele_adaboost_bordeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Mesure de la performance des modèles initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "auc_roc_adaboost_randomoversampler = roc_auc_score(y_test, y_proba_adaboost_randomoversampler)\n",
    "auc_pr_adaboost_randomoversampler = average_precision_score(y_test, y_proba_adaboost_randomoversampler)\n",
    "log_loss_value_adaboost_randomoversampler = log_loss(y_test, y_proba_adaboost_randomoversampler)\n",
    "precision_adaboost_randomoversampler = precision_score(y_test, y_pred_adaboost_randomoversampler)\n",
    "recall_adaboost_randomoversampler = recall_score(y_test, y_pred_adaboost_randomoversampler)\n",
    "f1_adaboost_randomoversampler = f1_score(y_test, y_pred_adaboost_randomoversampler)\n",
    "mcc_adaboost_randomoversampler = matthews_corrcoef(y_test, y_pred_adaboost_randomoversampler)\n",
    "balanced_acc_adaboost_randomoversampler = balanced_accuracy_score(y_test, y_pred_adaboost_randomoversampler)\n",
    "specificity_adaboost_randomoversampler = confusion_matrix(y_test, y_pred_adaboost_randomoversampler)[0, 0] / (confusion_matrix(y_test, y_pred_adaboost_randomoversampler)[0, 0] + confusion_matrix(y_test, y_pred_adaboost_randomoversampler)[0, 1])\n",
    "cohen_kappa_adaboost_randomoversampler = cohen_kappa_score(y_test, y_pred_adaboost_randomoversampler)\n",
    "\n",
    "auc_roc_adaboost_smote = roc_auc_score(y_test, y_proba_adaboost_smote)\n",
    "auc_pr_adaboost_smote = average_precision_score(y_test, y_proba_adaboost_smote)\n",
    "log_loss_value_adaboost_smote = log_loss(y_test, y_proba_adaboost_smote)\n",
    "precision_adaboost_smote = precision_score(y_test, y_pred_adaboost_smote)\n",
    "recall_adaboost_smote = recall_score(y_test, y_pred_adaboost_smote)\n",
    "f1_adaboost_smote = f1_score(y_test, y_pred_adaboost_smote)\n",
    "mcc_adaboost_smote = matthews_corrcoef(y_test, y_pred_adaboost_smote)\n",
    "balanced_acc_adaboost_smote = balanced_accuracy_score(y_test, y_pred_adaboost_smote)\n",
    "specificity_adaboost_smote = confusion_matrix(y_test, y_pred_adaboost_smote)[0, 0] / (confusion_matrix(y_test, y_pred_adaboost_smote)[0, 0] + confusion_matrix(y_test, y_pred_adaboost_smote)[0, 1])\n",
    "cohen_kappa_adaboost_smote = cohen_kappa_score(y_test, y_pred_adaboost_smote)\n",
    "\n",
    "auc_roc_adaboost_adasyn = roc_auc_score(y_test, y_proba_adaboost_adasyn)\n",
    "auc_pr_adaboost_adasyn = average_precision_score(y_test, y_proba_adaboost_adasyn)\n",
    "log_loss_value_adaboost_adasyn = log_loss(y_test, y_proba_adaboost_adasyn)\n",
    "precision_adaboost_adasyn = precision_score(y_test, y_pred_adaboost_adasyn)\n",
    "recall_adaboost_adasyn = recall_score(y_test, y_pred_adaboost_adasyn)\n",
    "f1_adaboost_adasyn = f1_score(y_test, y_pred_adaboost_adasyn)\n",
    "mcc_adaboost_adasyn = matthews_corrcoef(y_test, y_pred_adaboost_adasyn)\n",
    "balanced_acc_adaboost_adasyn = balanced_accuracy_score(y_test, y_pred_adaboost_adasyn)\n",
    "specificity_adaboost_adasyn = confusion_matrix(y_test, y_pred_adaboost_adasyn)[0, 0] / (confusion_matrix(y_test, y_pred_adaboost_adasyn)[0, 0] + confusion_matrix(y_test, y_pred_adaboost_adasyn)[0, 1])\n",
    "cohen_kappa_adaboost_adasyn = cohen_kappa_score(y_test, y_pred_adaboost_adasyn)\n",
    "\n",
    "auc_roc_adaboost_bordeline_smote = roc_auc_score(y_test, y_proba_adaboost_bordeline_smote)\n",
    "auc_pr_adaboost_bordeline_smote = average_precision_score(y_test, y_proba_adaboost_bordeline_smote)\n",
    "log_loss_value_adaboost_bordeline_smote = log_loss(y_test, y_proba_adaboost_bordeline_smote)\n",
    "precision_adaboost_bordeline_smote = precision_score(y_test, y_pred_adaboost_bordeline_smote)\n",
    "recall_adaboost_bordeline_smote = recall_score(y_test, y_pred_adaboost_bordeline_smote)\n",
    "f1_adaboost_bordeline_smote = f1_score(y_test, y_pred_adaboost_bordeline_smote)\n",
    "mcc_adaboost_bordeline_smote = matthews_corrcoef(y_test, y_pred_adaboost_bordeline_smote)\n",
    "balanced_acc_adaboost_bordeline_smote = balanced_accuracy_score(y_test, y_pred_adaboost_bordeline_smote)\n",
    "specificity_adaboost_bordeline_smote = confusion_matrix(y_test, y_pred_adaboost_bordeline_smote)[0, 0] / (confusion_matrix(y_test, y_pred_adaboost_bordeline_smote)[0, 0] + confusion_matrix(y_test, y_pred_adaboost_bordeline_smote)[0, 1])\n",
    "cohen_kappa_adaboost_bordeline_smote = cohen_kappa_score(y_test, y_pred_adaboost_bordeline_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_adaboost = pd.DataFrame({\n",
    "    \"AUC-ROC\" : [auc_roc_adaboost_randomoversampler, auc_roc_adaboost_smote, auc_roc_adaboost_adasyn, auc_roc_adaboost_bordeline_smote],\n",
    "    \"AUC-PR\" : [auc_pr_adaboost_randomoversampler, auc_pr_adaboost_smote, auc_pr_adaboost_adasyn, auc_pr_adaboost_bordeline_smote],\n",
    "    \"Log Loss\" : [log_loss_value_adaboost_randomoversampler, log_loss_value_adaboost_smote, log_loss_value_adaboost_adasyn, log_loss_value_adaboost_bordeline_smote],\n",
    "    \"Précision\" : [precision_adaboost_randomoversampler, precision_adaboost_smote, precision_adaboost_adasyn, precision_adaboost_bordeline_smote],\n",
    "    \"Rappel\" : [recall_adaboost_randomoversampler, recall_adaboost_smote, recall_adaboost_adasyn, recall_adaboost_bordeline_smote],\n",
    "    \"F1 Score\" : [f1_adaboost_randomoversampler, f1_adaboost_smote, f1_adaboost_adasyn, f1_adaboost_bordeline_smote],\n",
    "    \"MCC\" : [mcc_adaboost_randomoversampler, mcc_adaboost_smote, mcc_adaboost_adasyn, mcc_adaboost_bordeline_smote],\n",
    "    \"Accuracy\" : [balanced_acc_adaboost_randomoversampler, balanced_acc_adaboost_smote, balanced_acc_adaboost_adasyn, balanced_acc_adaboost_bordeline_smote],\n",
    "    \"Spécificité\" : [specificity_adaboost_randomoversampler, specificity_adaboost_smote, specificity_adaboost_adasyn, specificity_adaboost_bordeline_smote],\n",
    "    \"Kappa de Cohen\" : [cohen_kappa_adaboost_randomoversampler, cohen_kappa_adaboost_smote, cohen_kappa_adaboost_adasyn, cohen_kappa_adaboost_bordeline_smote]\n",
    "}, index=[\"AdaBoost - RandomOverSampler\", \"AdaBoost - SMOTE\", \"AdaBoost - ADASYN\", \"AdaBoost - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_adaboost = resultats_adaboost.round(3)\n",
    "resultats_adaboost    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "fpr_adaboost_randomoversampler, tpr_adaboost_randomoversampler, _ = roc_curve(y_test, y_proba_adaboost_randomoversampler)\n",
    "fpr_adaboost_smote, tpr_adaboost_smote, _ = roc_curve(y_test, y_proba_adaboost_smote)\n",
    "fpr_adaboost_adasyn, tpr_adaboost_adasyn, _ = roc_curve(y_test, y_proba_adaboost_adasyn)\n",
    "fpr_adaboost_bordeline_smote, tpr_adaboost_bordeline_smote, _ = roc_curve(y_test, y_proba_adaboost_bordeline_smote)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_adaboost_randomoversampler, tpr_adaboost_randomoversampler, label='AdaBoost avec RandomOverSampler')\n",
    "plt.plot(fpr_adaboost_smote, tpr_adaboost_smote, label='AdaBoost avec SMOTE')\n",
    "plt.plot(fpr_adaboost_adasyn, tpr_adaboost_adasyn, label='AdaBoost avec ADASYN')\n",
    "plt.plot(fpr_adaboost_bordeline_smote, tpr_adaboost_bordeline_smote, label='AdaBoost avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Ajustement des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement des hyperparamètres par validation croisée\n",
    "\n",
    "# Définition de la grille des hyperparamètres pour le AdaBoost\n",
    "\n",
    "param_grid = {\n",
    "    'adaboost__n_estimators': [50, 100, 200, 300, 400, 500], # Nombre d'arbres\n",
    "    'adaboost__learning_rate': [0.01, 0.05, 0.1, 0.5], # Taux d'apprentissage\n",
    "}\n",
    "\n",
    "# Choix des métriques adaptées\n",
    "scoring = {\n",
    "    'F1': make_scorer(f1_score), \n",
    "    'Recall': make_scorer(recall_score),\n",
    "    'AUC': 'roc_auc'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "\n",
    "pipelines = {\n",
    "    'oversampler': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('oversampler', RandomOverSampler()), \n",
    "        ('adaboost', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('smote', SMOTE()), \n",
    "        ('adaboost', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'adasyn': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('adasyn', ADASYN()), \n",
    "        ('adaboost', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'bordeline_smote': Pipeline([\n",
    "        ('std_scaler', StandardScaler()), \n",
    "        ('bordeline_smote', BorderlineSMOTE()), \n",
    "        ('adaboost', AdaBoostClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On stocke les meilleurs paramètres pour chaque modèle\n",
    "best_params = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit='F1', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    print(f\"Meilleurs paramètres pour {name} : {grid_search.best_params_}\")\n",
    "\n",
    "# On stocke les meilleurs modèles\n",
    "best_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    best_models[name] = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions\n",
    "\n",
    "y_proba_adaboost_randomoversampler_cv = best_models['oversampler'].predict_proba(X_test)[:, 1]\n",
    "y_proba_adaboost_smote_cv = best_models['smote'].predict_proba(X_test)[:, 1]\n",
    "y_proba_adaboost_adasyn_cv = best_models['adasyn'].predict_proba(X_test)[:, 1]\n",
    "y_proba_adaboost_bordeline_smote_cv = best_models['bordeline_smote'].predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Mesure de la performance des modèles optimisés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "\n",
    "auc_roc_adaboost_randomoversampler_cv = roc_auc_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "auc_pr_adaboost_randomoversampler_cv = average_precision_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "log_loss_value_adaboost_randomoversampler_cv = log_loss(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "precision_adaboost_randomoversampler_cv = precision_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "recall_adaboost_randomoversampler_cv = recall_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "f1_adaboost_randomoversampler_cv = f1_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "mcc_adaboost_randomoversampler_cv = matthews_corrcoef(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "balanced_acc_adaboost_randomoversampler_cv = balanced_accuracy_score(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "\n",
    "auc_roc_adaboost_smote_cv = roc_auc_score(y_test, y_proba_adaboost_smote_cv)\n",
    "auc_pr_adaboost_smote_cv = average_precision_score(y_test, y_proba_adaboost_smote_cv)\n",
    "log_loss_value_adaboost_smote_cv = log_loss(y_test, y_proba_adaboost_smote_cv)\n",
    "precision_adaboost_smote_cv = precision_score(y_test, y_proba_adaboost_smote_cv)\n",
    "recall_adaboost_smote_cv = recall_score(y_test, y_proba_adaboost_smote_cv)\n",
    "f1_adaboost_smote_cv = f1_score(y_test, y_proba_adaboost_smote_cv)\n",
    "mcc_adaboost_smote_cv = matthews_corrcoef(y_test, y_proba_adaboost_smote_cv)\n",
    "balanced_acc_adaboost_smote_cv = balanced_accuracy_score(y_test, y_proba_adaboost_smote_cv)\n",
    "\n",
    "auc_roc_adaboost_adasyn_cv = roc_auc_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "auc_pr_adaboost_adasyn_cv = average_precision_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "log_loss_value_adaboost_adasyn_cv = log_loss(y_test, y_proba_adaboost_adasyn_cv)\n",
    "precision_adaboost_adasyn_cv = precision_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "recall_adaboost_adasyn_cv = recall_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "f1_adaboost_adasyn_cv = f1_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "mcc_adaboost_adasyn_cv = matthews_corrcoef(y_test, y_proba_adaboost_adasyn_cv)\n",
    "balanced_acc_adaboost_adasyn_cv = balanced_accuracy_score(y_test, y_proba_adaboost_adasyn_cv)\n",
    "\n",
    "auc_roc_adaboost_bordeline_smote_cv = roc_auc_score(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "auc_pr_adaboost_bordeline_smote_cv = average_precision_score(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "log_loss_value_adaboost_bordeline_smote_cv = log_loss(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "precision_adaboost_bordeline_smote_cv = precision_score(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "recall_adaboost_bordeline_smote_cv = recall_score(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "f1_adaboost_bordeline_smote_cv = f1_score(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "mcc_adaboost_bordeline_smote_cv = matthews_corrcoef(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "balanced_acc_adaboost_bordeline_smote_cv = balanced_accuracy_score(y_test, y_proba_adaboost_bordeline_smote_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "\n",
    "resultats_adaboost_cv = pd.DataFrame({\n",
    "    \"AUC-ROC\": [auc_roc_adaboost_randomoversampler_cv, auc_roc_adaboost_smote_cv, auc_roc_adaboost_adasyn_cv, auc_roc_adaboost_bordeline_smote_cv],\n",
    "    \"AUC-PR\": [auc_pr_adaboost_randomoversampler_cv, auc_pr_adaboost_smote_cv, auc_pr_adaboost_adasyn_cv, auc_pr_adaboost_bordeline_smote_cv],\n",
    "    \"Log Loss\": [log_loss_value_adaboost_randomoversampler_cv, log_loss_value_adaboost_smote_cv, log_loss_value_adaboost_adasyn_cv, log_loss_value_adaboost_bordeline_smote_cv],\n",
    "    \"Précision\": [precision_adaboost_randomoversampler_cv, precision_adaboost_smote_cv, precision_adaboost_adasyn_cv, precision_adaboost_bordeline_smote_cv],\n",
    "    \"Rappel\": [recall_adaboost_randomoversampler_cv, recall_adaboost_smote_cv, recall_adaboost_adasyn_cv, recall_adaboost_bordeline_smote_cv],\n",
    "    \"F1 Score\": [f1_adaboost_randomoversampler_cv, f1_adaboost_smote_cv, f1_adaboost_adasyn_cv, f1_adaboost_bordeline_smote_cv],\n",
    "    \"MCC\": [mcc_adaboost_randomoversampler_cv, mcc_adaboost_smote_cv, mcc_adaboost_adasyn_cv, mcc_adaboost_bordeline_smote_cv],\n",
    "    \"Accuracy\": [balanced_acc_adaboost_randomoversampler_cv, balanced_acc_adaboost_smote_cv, balanced_acc_adaboost_adasyn_cv, balanced_acc_adaboost_bordeline_smote_cv]\n",
    "}, index=[\"AdaBoost - RandomOverSampler\", \"AdaBoost - SMOTE\", \"AdaBoost - ADASYN\", \"AdaBoost - BorderlineSMOTE\"])\n",
    "\n",
    "# Arrondir les résultats à 3 chiffres après la virgule\n",
    "resultats_adaboost_cv = resultats_adaboost_cv.round(3)\n",
    "resultats_adaboost_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "\n",
    "fpr_adaboost_randomoversampler_cv, tpr_adaboost_randomoversampler_cv, _ = roc_curve(y_test, y_proba_adaboost_randomoversampler_cv)\n",
    "fpr_adaboost_smote_cv, tpr_adaboost_smote_cv, _ = roc_curve(y_test, y_proba_adaboost_smote_cv)\n",
    "fpr_adaboost_adasyn_cv, tpr_adaboost_adasyn_cv, _ = roc_curve(y_test, y_proba_adaboost_adasyn_cv)\n",
    "fpr_adaboost_bordeline_smote_cv, tpr_adaboost_bordeline_smote_cv, _ = roc_curve(y_test, y_proba_adaboost_bordeline_smote_cv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_adaboost_randomoversampler_cv, tpr_adaboost_randomoversampler_cv, label='AdaBoost avec RandomOverSampler')\n",
    "plt.plot(fpr_adaboost_smote_cv, tpr_adaboost_smote_cv, label='AdaBoost avec SMOTE')\n",
    "plt.plot(fpr_adaboost_adasyn_cv, tpr_adaboost_adasyn_cv, label='AdaBoost avec ADASYN')\n",
    "plt.plot(fpr_adaboost_bordeline_smote_cv, tpr_adaboost_bordeline_smote_cv, label='AdaBoost avec BorderlineSMOTE')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "\n",
    "# Récupération de l'importance des variables pour chaque modèle\n",
    "importances_adaboost_randomoversampler = best_models['oversampler'].named_steps['adaboost'].feature_importances_\n",
    "importances_adaboost_smote = best_models['smote'].named_steps['adaboost'].feature_importances_\n",
    "importances_adaboost_adasyn = best_models['adasyn'].named_steps['adaboost'].feature_importances_\n",
    "importances_adaboost_bordeline_smote = best_models['bordeline_smote'].named_steps['adaboost'].feature_importances_\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "resultats_importance_variables_adaboost_randomoversampler = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_adaboost_randomoversampler\n",
    "})\n",
    "\n",
    "resultats_importance_variables_adaboost_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_adaboost_smote\n",
    "})\n",
    "\n",
    "resultats_importance_variables_adaboost_adasyn = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_adaboost_adasyn\n",
    "})\n",
    "\n",
    "resultats_importance_variables_adaboost_bordeline_smote = pd.DataFrame({\n",
    "    \"Variable\": noms_variables,\n",
    "    \"Importance\": importances_adaboost_bordeline_smote\n",
    "})\n",
    "\n",
    "# Affichage des résultats\n",
    "resultats_importance_variables_adaboost_randomoversampler = resultats_importance_variables_adaboost_randomoversampler.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_adaboost_smote = resultats_importance_variables_adaboost_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_adaboost_adasyn = resultats_importance_variables_adaboost_adasyn.sort_values(by=\"Importance\", ascending=False)\n",
    "resultats_importance_variables_adaboost_bordeline_smote = resultats_importance_variables_adaboost_bordeline_smote.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "resultats_importance_variables_adaboost_randomoversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_adaboost_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_adaboost_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_importance_variables_adaboost_bordeline_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'importance des variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_adaboost_randomoversampler['Variable'], resultats_importance_variables_adaboost_randomoversampler['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le AdaBoost avec RandomOverSampler')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_adaboost_smote['Variable'], resultats_importance_variables_adaboost_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le AdaBoost avec SMOTE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_adaboost_adasyn['Variable'], resultats_importance_variables_adaboost_adasyn['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le AdaBoost avec ADASYN')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(resultats_importance_variables_adaboost_bordeline_smote['Variable'], resultats_importance_variables_adaboost_bordeline_smote['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des variables pour le AdaBoost avec BorderlineSMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles poour sélectionner le meilleur \n",
    "\n",
    "resultats = pd.concat([resultats_reg_log_cv, \n",
    "                       resultats_arbres_cv, \n",
    "                       resultats_rf_cv, \n",
    "                       resultats_gb_cv, \n",
    "                       resultats_adaboost_cv], axis=0)\n",
    "resultats = resultats.sort_values(by='F1 Score', ascending=False)\n",
    "resultats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
